{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install GPUtil\n","metadata":{"execution":{"iopub.status.busy":"2023-05-31T21:05:09.732553Z","iopub.execute_input":"2023-05-31T21:05:09.733013Z","iopub.status.idle":"2023-05-31T21:05:24.342163Z","shell.execute_reply.started":"2023-05-31T21:05:09.732971Z","shell.execute_reply":"2023-05-31T21:05:24.341057Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting GPUtil\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: GPUtil\n  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7409 sha256=7fae8ce7ccfdf8f138d4781c76ee193828d99709af9887ac69ac102364445133\n  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\nSuccessfully built GPUtil\nInstalling collected packages: GPUtil\nSuccessfully installed GPUtil-1.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import json\nimport math\nimport argparse\nimport torch\nimport logging\nimport numpy as np\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom os import path\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\nimport copy\nfrom operator import attrgetter\nfrom typing import Dict, List, Optional, Tuple, Set, Union, Iterable\nimport collections\nfrom torch import Tensor\nfrom torch.nn import functional as F\nfrom scipy.stats import rankdata\nimport gc \nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-31T21:05:24.345004Z","iopub.execute_input":"2023-05-31T21:05:24.345692Z","iopub.status.idle":"2023-05-31T21:05:30.175925Z","shell.execute_reply.started":"2023-05-31T21:05:24.345646Z","shell.execute_reply":"2023-05-31T21:05:30.174946Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Utils\ndef tokenize_constraints(tokenizer, raw_cts):\n    def tokenize(phrase):\n        tokens = tokenizer.tokenize(phrase)\n        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n        return token_ids, True\n    return [[list(map(tokenize, clause)) for clause in ct] for ct in raw_cts]\n\ndef report_gpu(): \n    print(torch.cuda.list_gpu_processes())\n    gc.collect() \n    torch.cuda.empty_cache()\n    \ndef free_gpu_cache():\n#     print(\"Initial GPU Usage\")\n    \n#     gpu_usage()                             \n    gc.collect() \n    torch.cuda.empty_cache()\n\n#     cuda.select_device(0)\n#     cuda.close()\n#     cuda.select_device(0)\n\n#     print(\"GPU Usage after emptying the cache\")\n#     gpu_usage()","metadata":{"execution":{"iopub.status.busy":"2023-05-31T21:05:30.177348Z","iopub.execute_input":"2023-05-31T21:05:30.177981Z","iopub.status.idle":"2023-05-31T21:05:30.186210Z","shell.execute_reply.started":"2023-05-31T21:05:30.177943Z","shell.execute_reply":"2023-05-31T21:05:30.184422Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# lexical constraints\nlogger = logging.getLogger(__name__)\n\nPhrase = List[int]\nLiteral = Tuple[Phrase, bool]\n# Represents a list of raw constraints for a sentence. Each constraint is a list of target-word IDs.\nRawConstraintList = List[Phrase]\nClauseConstraintList = List[List[Literal]]\n\n\nclass Trie:\n    \"\"\"\n    Represents a set of phrasal constraints for an input sentence.\n    These are organized into a trie.\n    \"\"\"\n    def __init__(self,\n                 raw_phrases: Optional[RawConstraintList] = None,\n                 parent_arc: int = None,\n                 parent_trie: 'Trie' = None) -> None:\n        self.final_ids = set()  # type: Set[int]\n        self.children = {}  # type: Dict[int,'Trie']\n        self.parent_arc = parent_arc\n        self.parent_trie = parent_trie\n\n        if raw_phrases:\n            for phrase in raw_phrases:\n                self.add_phrase(phrase)\n\n    def add_phrase(self,\n                   phrase: List[int]) -> None:\n        \"\"\"\n        Recursively adds a phrase to this trie node.\n\n        :param phrase: A list of word IDs to add to this trie node.\n        \"\"\"\n        if len(phrase) == 1:\n            self.final_ids.add(phrase[0])\n        else:\n            next_word = phrase[0]\n            if next_word not in self.children:\n                self.children[next_word] = Trie(parent_arc=next_word, parent_trie=self)\n            self.step(next_word).add_phrase(phrase[1:])\n\n    def delete_phrase(self,\n                      phrase: List[int]) -> None:\n        \"\"\"\n        Recursively deletes a phrase to this trie node.\n\n        :param phrase: A list of word IDs to delete in this trie node.\n        \"\"\"\n        if len(phrase) == 1:\n            assert phrase[0] in self.final_ids, f\"Trie {str(self)} \\nDo not contain {phrase}\"\n            self.final_ids.remove(phrase[0])\n        else:\n            next_word = phrase[0]\n            assert next_word in self.children.keys(), f\"Trie {str(self)} \\nDo not contain {phrase}\"\n            self.step(next_word).delete_phrase(phrase[1:])\n\n        # Move the arc to an empty node to final_ids of its parent\n        for arc in list(self.children):\n            if len(self.children[arc]) == 0:\n                self.children.pop(arc)\n\n    def check_phrase(self,\n                     phrase: List[int]) -> bool:\n        \"\"\"\n        Check whether a phrase is in this trie.\n\n        :param phrase: A list of word IDs to check existence.\n        \"\"\"\n        if len(phrase) == 1:\n            return phrase[0] in self.final_ids\n        else:\n            next_word = phrase[0]\n            if next_word in self.children:\n                return self.step(next_word).check_phrase(phrase[1:])\n            return False\n\n    def trace_phrase(self,\n                     word_id: int) -> List[int]:\n        \"\"\"\n        Recursively backward to get word ids in a phrase.\n\n        :param word_id: The last word IDs in phrase.\n        \"\"\"\n        assert word_id in self.final_ids, f\"{word_id} does not in trie node {self.final_ids}\"\n        phrase = self.trace_arcs()\n        phrase.append(word_id)\n        return phrase\n\n    def trace_arcs(self,) -> List[int]:\n        \"\"\"\n        Recursively backward to get arc to ancestor\n        \"\"\"\n        arcs = []\n        parent_trie, parent_arc = self.parent_trie, self.parent_arc\n        while parent_trie is not None:\n            arcs.append(parent_arc)\n            parent_arc = parent_trie.parent_arc\n            parent_trie = parent_trie.parent_trie\n        arcs.reverse()\n        return arcs\n\n    def __str__(self) -> str:\n        s = f'({list(self.final_ids)}'\n        for child_id in self.children.keys():\n            s += f' -> {child_id} {self.children[child_id]}'\n        s += ')'\n        return s\n\n    def __len__(self) -> int:\n        \"\"\"\n        Returns the number of phrases represented in the trie.\n        \"\"\"\n        phrase_count = len(self.final_ids)\n        for child in self.children.values():\n            phrase_count += len(child)\n        return phrase_count\n\n    def step(self, word_id: int) -> Optional['Trie']:\n        \"\"\"\n        Returns the child node along the requested arc.\n\n        :param word_id: requested arc.\n        :return: The child node along the requested arc, or None if no such arc exists.\n        \"\"\"\n        return self.children.get(word_id, None)\n\n    def descend(self,\n              arcs: List[int]) -> Optional['Trie']:\n        pointer = self\n        for arc in arcs:\n            if pointer is None:\n                break\n            pointer = pointer.step(word_id=arc)\n        return pointer\n\n    def final(self) -> Set[int]:\n        \"\"\"\n        Returns the set of final ids at this node.\n\n        :return: The set of word IDs that end a constraint at this state.\n        \"\"\"\n        return self.final_ids\n\n\nclass NegativeState:\n    \"\"\"\n    Represents the state of a hypothesis in the AvoidTrie.\n    The offset is used to return actual positions in the one-dimensionally-resized array that\n    get set to infinity.\n\n    :param avoid_trie: The trie containing the phrases to avoid.\n    :param state: The current state (defaults to root).\n    \"\"\"\n    def __init__(self,\n                 avoid_trie: Trie,\n                 state: List[Trie] = None) -> None:\n\n        self.root = avoid_trie\n        self.state = state if state else [self.root]\n\n    def consume(self, word_id: int) -> 'NegativeState':\n        \"\"\"\n        Consumes a word, and updates the state based on it. Returns new objects on a state change.\n\n        The next state for a word can be tricky. Here are the cases:\n        (1) If the word is found in our set of outgoing child arcs, we take that transition.\n        (2) If the word is not found, and we are not in the root state, we need to reset.\n            This means we pretend we were in the root state, and see if we can take a step\n        (3) Otherwise, if we are not already in the root state (i.e., we were partially through\n            the trie), we need to create a new object whose state is the root state\n        (4) Finally, if we couldn't advance and were already in the root state, we can reuse\n            this object.\n\n        :param word_id: The word that was just generated.\n        \"\"\"\n        new_state = []\n        for state in set(self.state + [self.root]):\n            if word_id in state.children:\n                new_state.append(state.step(word_id))\n\n        if new_state:\n            return NegativeState(self.root, new_state)\n        else:\n            if len(self.state) == 1 and self.root == self.state[0]:\n                return self\n            else:\n                return NegativeState(self.root, [self.root])\n\n    def avoid(self) -> Set[int]:\n        \"\"\"\n        Returns a set of word IDs that should be avoided. This includes the set of final states from the\n        root node, which are single tokens that must never be generated.\n\n        :return: A set of integers representing words that must not be generated next by this hypothesis.\n        \"\"\"\n        return self.root.final().union(*[state.final() for state in self.state])\n\n    def __str__(self) -> str:\n        return str(self.state)\n\n\nclass NegativeBatch:\n    \"\"\"\n    Represents a set of phrasal constraints for all items in the batch.\n    For each hypotheses, there is an AvoidTrie tracking its state.\n\n    :param beam_size: The beam size.\n    :param avoid_list: The list of lists (raw phrasal constraints as IDs, one for each item in the batch).\n    \"\"\"\n    def __init__(self,\n                 beam_size: int,\n                 avoid_list: Optional[List[RawConstraintList]] = None) -> None:\n\n        self.avoid_states = []  # type: List[NegativeState]\n\n        # Store the sentence-level tries for each item in their portions of the beam\n        if avoid_list is not None:\n            for literal_phrases in avoid_list:\n                self.avoid_states += [NegativeState(Trie(literal_phrases))] * beam_size\n\n    def reorder(self, indices: torch.Tensor) -> None:\n        \"\"\"\n        Reorders the avoid list according to the selected row indices.\n        This can produce duplicates, but this is fixed if state changes occur in consume().\n\n        :param indices: An mx.nd.NDArray containing indices of hypotheses to select.\n        \"\"\"\n        if self.avoid_states:\n            self.avoid_states = [self.avoid_states[x] for x in indices.numpy()]\n\n    def consume(self, word_ids: torch.Tensor) -> None:\n        \"\"\"\n        Consumes a word for each trie, updating respective states.\n\n        :param word_ids: The set of word IDs.\n        \"\"\"\n        word_ids = word_ids.numpy().tolist()\n        for i, word_id in enumerate(word_ids):\n            if self.avoid_states:\n                self.avoid_states[i] = self.avoid_states[i].consume(word_id)\n\n    def avoid(self) -> Tuple[Tuple[int], Tuple[int]]:\n        \"\"\"\n        Assembles a list of per-hypothesis words to avoid. The indices are (x, y) pairs into the scores\n        array, which has dimensions (beam_size, target_vocab_size). These values are then used by the caller\n        to set these items to np.inf so they won't be selected. Words to be avoided are selected by\n        consulting both the global trie of phrases and the sentence-specific one.\n\n        :return: Two lists of indices: the x coordinates and y coordinates.\n        \"\"\"\n        to_avoid = set()  # type: Set[Tuple[int, int]]\n        for i, state in enumerate(self.avoid_states):\n            for word_id in state.avoid():\n                to_avoid.add((i, word_id))\n\n        return tuple(zip(*to_avoid))  # type: ignore\n\n\nclass PositiveState:\n    \"\"\"\n    Represents a set of words and phrases that must appear in the output.\n    The offset is used to return actual positions in the one-dimensionally-resized array that\n    get set to infinity.\n\n    :param positive_trie: The trie containing the phrases to appear.\n    :param state: The current state (defaults to root).\n    \"\"\"\n    def __init__(self,\n                 positive_trie: Trie,\n                 state: List[Trie] = None,\n                 met_phrases: RawConstraintList = None) -> None:\n\n        self.root = positive_trie\n        self.state = state if state else [self.root]\n        self.met_phrases = met_phrases\n\n    def __str__(self):\n        s = f'Root: {self.root}\\nState: ['\n        for state in self.state:\n            s += f'{state}, '\n        s += f']\\nMet_phrases: {self.met_phrases}'\n        return s\n\n    def allowed(self) -> Set[int]:\n        \"\"\"\n        Returns the set of constrained words that could follow this one.\n        For unfinished phrasal constraints, it is the next word in the phrase.\n        In other cases, it is the list of all unmet constraints.\n        If all constraints are met, an empty set is returned.\n\n        :return: The ID of the next required word, or -1 if any word can follow\n        \"\"\"\n        allow = self.root.final().union(*[state.final() for state in self.state])\n        allow |= set(self.root.children.keys()).union(*[set(state.children.keys()) for state in self.state])\n        return allow\n\n    def advance(self, word_id: int) -> 'PositiveState':\n        \"\"\"\n        Updates the constraints object based on advancing on word_id.\n        There is a complication, in that we may have started but not\n        yet completed a multi-word constraint.  We need to allow constraints\n        to be added as unconstrained words, so if the next word is\n        invalid, we must \"back out\" of the current (incomplete) phrase,\n        re-setting all of its words as unmet.\n\n        :param word_id: The word ID to advance on.\n        :return: A deep copy of the object, advanced on word_id.\n        \"\"\"\n        new_state, met_phrases = [], []\n        for state in set(self.state + [self.root]):\n            if word_id in state.children:\n                new_state.append(state.step(word_id))\n            if word_id in state.final_ids:\n                met_phrases.append(state.trace_phrase(word_id))\n\n        if new_state:\n            return PositiveState(self.root, new_state, met_phrases if met_phrases else None)\n        else:\n            if len(self.state) == 1 and self.root == self.state[0] and not met_phrases:\n                return self\n            else:\n                return PositiveState(self.root, [self.root], met_phrases if met_phrases else None)\n\n\nclass Clause:\n    \"\"\"\n    Object used to hold clause.\n\n    :param idx: The id of this clause.\n    :param positive: The positive constraints in this clause.\n    :param negative: The soft negative constraints in this clause.\n    :param satisfy: whether this clause is satisfied\n    \"\"\"\n\n    __slots__ = ('idx', 'positive', 'negative', 'satisfy')\n\n    def __init__(self,\n                 idx: int,\n                 positive: List[Phrase],\n                 negative: List[Phrase],\n                 satisfy: float) -> None:\n        self.idx = idx\n        self.positive = positive\n        self.negative = negative\n        self.satisfy = satisfy\n\n    def __str__(self):\n        return f'clause(id={self.idx}, positive={self.positive}, negative={self.negative}, satisfy={self.satisfy})'\n\n\ndef is_prefix(pref: List[int],\n              phrase: List[int]):\n    if not pref:\n        return False\n    return pref == phrase[:len(pref)]\n\n\nclass ConstrainedHypothesis:\n    \"\"\"\n    Keep track of positive and negative constraint\n\n    hard negative constraint will not be generated in any cases\n    soft negative constraint might be generated in some case due to OR gate in clause\n    positive constraints will be encourage to appear\n\n    :param constraint_list: A list of clause constraints (each represented as a list of literals).\n    \"\"\"\n    def __init__(self,\n                 constraint_list: ClauseConstraintList,\n                 eos_id: Union[int, list]\n                 ) -> None:\n        self.eos_id = eos_id if isinstance(eos_id, list) else [eos_id]\n        self.clauses = []  # type: List[Clause]\n\n        hard_neg_pool, soft_neg_pool, pos_pool = [], [], []  # type: RawConstraintList\n        for idx, clause in enumerate(constraint_list):\n            if not clause:\n                continue\n            pos_phrases, neg_phrases = [l[0] for l in clause if l[1]], [l[0] for l in clause if not l[1]]\n            # clause contains single negative literal\n            if not pos_phrases and len(neg_phrases) == 1:\n                hard_neg_pool.extend(neg_phrases)\n                #self.clauses.append(Clause(idx=idx, positive=[], negative=neg_phrases, satisfy=True))\n            # clause contains multiple negative literals or both negative and positive literals\n            elif neg_phrases:\n                soft_neg_pool.extend(neg_phrases)\n                self.clauses.append(Clause(idx=idx, positive=pos_phrases, negative=neg_phrases, satisfy=True))\n            # clause contains only positive literals\n            elif pos_phrases and not neg_phrases:\n                pos_pool.extend(pos_phrases)\n                self.clauses.append(Clause(idx=idx, positive=pos_phrases, negative=[], satisfy=False))\n            else:\n                import ipdb\n                ipdb.set_trace()\n                raise ValueError(f'Invalid state {clause}, should not be reached')\n\n        self.hard_negative_state = NegativeState(Trie(hard_neg_pool)) if hard_neg_pool else None\n        self.soft_negative_state = NegativeState(Trie(soft_neg_pool)) if soft_neg_pool else None\n        self.positive_state = PositiveState(Trie(pos_pool)) if pos_pool else None\n\n        self.orders = []\n        self.in_process = None\n        self.max_process = 0\n\n    def __len__(self) -> int:\n        \"\"\"\n        :return: The number of constraints.\n        \"\"\"\n        return len(self.clauses)\n\n    def __str__(self) -> str:\n        return '\\n'.join([str(c) for c in self.clauses])\n\n    def size(self) -> int:\n        \"\"\"\n        :return: the number of constraints\n        \"\"\"\n        return len(self.clauses)\n\n    def num_met(self) -> int:\n        \"\"\"\n        :return: the number of constraints that have been met.\n        \"\"\"\n        if not self.clauses:\n            return 0\n        return sum([int(c.satisfy) for c in self.clauses])\n\n    def met_order(self) -> tuple:\n        \"\"\"\n        :return: the number of constraints that have been met.\n        \"\"\"\n        return tuple(sorted(self.orders))\n\n    def clause_in_process(self) -> tuple:\n        \"\"\"\n        :return: the index of clause that's in generation.\n        \"\"\"\n        return tuple(self.in_process)\n\n    def num_needed(self) -> int:\n        \"\"\"\n        :return: the number of un-met constraints.\n        \"\"\"\n        return self.size() - self.num_met()\n\n    def finished(self) -> bool:\n        \"\"\"\n        Return true if all the constraints have been met.\n\n        :return: True if all the constraints are met.\n        \"\"\"\n        return self.num_needed() == 0\n\n    def is_valid(self, wordid: int) -> bool:\n        \"\"\"\n        Ensures </s> is only generated when the hypothesis is completed.\n\n        :param wordid: The wordid to validate.\n        :return: True if all constraints are already met or the word ID is not the EOS id.\n        \"\"\"\n        return self.finished() or wordid not in self.eos_id\n\n    def avoid(self) -> Set[int]:\n        banned = self.hard_negative_state.avoid() if self.hard_negative_state is not None else set()\n        return banned\n\n    def eos(self) -> list:\n        \"\"\"\n        :return: Return EOS id.\n        \"\"\"\n        return self.eos_id\n\n    def advance(self, word_id: int) -> 'ConstrainedHypothesis':\n        \"\"\"\n        Updates the constraints object based on advancing on word_id.\n        If one of literals in a clause is satisfied, we mark this clause as satisfied\n\n        :param word_id: The word ID to advance on.\n        \"\"\"\n        obj = copy.deepcopy(self)\n\n        if obj.soft_negative_state is not None:\n            raise NotImplementedError\n\n        if obj.hard_negative_state is not None:\n            obj.hard_negative_state = obj.hard_negative_state.consume(word_id)\n\n        if obj.positive_state is not None:\n            temp_pos_state = obj.positive_state.advance(word_id)\n            if temp_pos_state.met_phrases is not None:\n                # get newly satisfied positive literals\n                phrases_to_delete = []\n                newly_met_clause = set()\n                for phrase in temp_pos_state.met_phrases:\n                    for clause in obj.clauses:\n                        if not clause.satisfy and phrase in clause.positive:\n                            phrases_to_delete.extend(clause.positive)\n                            clause.satisfy = True\n                            assert clause.idx not in obj.orders, 'clause has already satisfied, impossible state'\n                            newly_met_clause.add(clause.idx)\n                obj.orders.extend(sorted(newly_met_clause))\n\n                # delete newly satisfied literals from positive trie state\n                new_root = copy.deepcopy(temp_pos_state.root)\n                phrases_to_delete = [list(i) for i in set(map(tuple, phrases_to_delete))]\n                for phrase in phrases_to_delete:\n                    if new_root.check_phrase(phrase):\n                        new_root.delete_phrase(phrase)\n                new_trie_states = set()\n                for state in temp_pos_state.state:\n                    # pointer at root state\n                    if state.parent_trie is None:\n                        new_trie_states.add(new_root)\n                    else:\n                        trace = state.trace_arcs()\n                        new_state = new_root.descend(trace)\n                        if new_state is not None:\n                            new_trie_states.add(new_state)\n                obj.positive_state = PositiveState(positive_trie=new_root, state=list(new_trie_states))\n            else:\n                obj.positive_state = temp_pos_state\n\n            history = [s.trace_arcs() for s in obj.positive_state.state]\n            newly_in_process = set()\n            max_process = 0\n            for phrase in history:\n                for clause in obj.clauses:\n                    phrase_in_process = [c for c in clause.positive if is_prefix(phrase, c)]\n                    if not clause.satisfy and bool(phrase_in_process):\n                        process_portion = len(phrase) / min([len(x) for x in phrase_in_process])\n                        max_process = max(max_process, process_portion)\n                        assert clause.idx not in obj.orders, 'clause has already satisfied, impossible state'\n                        newly_in_process.add(clause.idx)\n            obj.in_process = sorted(newly_in_process)\n            obj.max_process = max_process\n        return obj\n\n\ndef init_batch(raw_constraints: List[ClauseConstraintList],\n               beam_size: int,\n               eos_id: Union[int, list]) -> List[Optional[ConstrainedHypothesis]]:\n    \"\"\"\n    :param raw_constraints: The list of clause constraints.\n    :param beam_size: The beam size.\n    :param eos_id: The target-language vocabulary ID of the EOS symbol.\n    :return: A list of ConstrainedHypothesis objects (shape: (batch_size * beam_size,)).\n    \"\"\"\n    constraints_list = [None] * (len(raw_constraints) * beam_size)  # type: List[Optional[ConstrainedHypothesis]]\n    for i, raw_list in enumerate(raw_constraints):\n        hyp = ConstrainedHypothesis(raw_list, eos_id)\n        idx = i * beam_size\n        constraints_list[idx:idx + beam_size] = [copy.deepcopy(hyp) for _ in range(beam_size)]\n    return constraints_list\n\n\nclass ConstrainedCandidate:\n    \"\"\"\n    Object used to hold candidates for the beam in topk().\n\n    :param row: The row in the scores matrix.\n    :param col: The column (word ID) in the scores matrix.\n    :param score: the associated accumulated score.\n    :param hypothesis: The ConstrainedHypothesis containing information about met constraints.\n    \"\"\"\n\n    __slots__ = ('row', 'col', 'score', 'hypothesis', 'rank')\n\n    def __init__(self,\n                 row: int,\n                 col: int,\n                 score: float,\n                 hypothesis: ConstrainedHypothesis,\n                 rank: float = None,) -> None:\n        self.row = row\n        self.col = col\n        self.score = score\n        self.hypothesis = hypothesis\n        self.rank = rank\n\n    def __hash__(self):\n        return hash((self.row, self.col))\n\n    def __eq__(self, other):\n        return self.row == other.row and self.col == other.col\n\n    def __str__(self):\n        return '({}, {}, {}, {})'.format(self.row, self.col, self.score, self.hypothesis.num_met())\n\n\nif __name__ == '__main__':\n    clauses = [[[([3, 4, 5], True), ([3, 4], True), ([4, 5], True)], [([3, 4], True), ([6], True), ([7], True)]],\n               [[([6], True), ([6, 7], True), ([6, 7, 8], True)], [([6, 9], True), ([6, 4, 9], True)]],\n               [[([3, 4, 5], True)], [([3, 4], True)], [([4, 5], True)]],\n               [[([3, 4], True)], [([2, 3, 5], True)], [([6, 5], True)]]]\n\n    constraints = init_batch(raw_constraints=clauses,\n                             beam_size=1,\n                             eos_id=0)\n\n    constraint = constraints[2]\n    print(constraint)\n    print(constraints)\n    print()\n    for w in [2, 3, 4, 5]:\n        constraint = constraint.advance(w)\n        print(constraint)\n        print(constraint.positive_state)\n        print(constraint.positive_state.allowed())\n        print(constraint.met_order())\n        print(constraint.clause_in_process())\n        print()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-31T21:05:30.187965Z","iopub.execute_input":"2023-05-31T21:05:30.188613Z","iopub.status.idle":"2023-05-31T21:05:30.273290Z","shell.execute_reply.started":"2023-05-31T21:05:30.188578Z","shell.execute_reply":"2023-05-31T21:05:30.272275Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"clause(id=0, positive=[[3, 4, 5]], negative=[], satisfy=False)\nclause(id=1, positive=[[3, 4]], negative=[], satisfy=False)\nclause(id=2, positive=[[4, 5]], negative=[], satisfy=False)\n[<__main__.ConstrainedHypothesis object at 0x795235ea0640>, <__main__.ConstrainedHypothesis object at 0x795235ea9db0>, <__main__.ConstrainedHypothesis object at 0x795235eaa1d0>, <__main__.ConstrainedHypothesis object at 0x795235eaa7a0>]\n\nclause(id=0, positive=[[3, 4, 5]], negative=[], satisfy=False)\nclause(id=1, positive=[[3, 4]], negative=[], satisfy=False)\nclause(id=2, positive=[[4, 5]], negative=[], satisfy=False)\nRoot: ([] -> 3 ([4] -> 4 ([5])) -> 4 ([5]))\nState: [([] -> 3 ([4] -> 4 ([5])) -> 4 ([5])), ]\nMet_phrases: None\n{3, 4}\n()\n()\n\nclause(id=0, positive=[[3, 4, 5]], negative=[], satisfy=False)\nclause(id=1, positive=[[3, 4]], negative=[], satisfy=False)\nclause(id=2, positive=[[4, 5]], negative=[], satisfy=False)\nRoot: ([] -> 3 ([4] -> 4 ([5])) -> 4 ([5]))\nState: [([4] -> 4 ([5])), ]\nMet_phrases: None\n{3, 4}\n()\n(0, 1)\n\nclause(id=0, positive=[[3, 4, 5]], negative=[], satisfy=False)\nclause(id=1, positive=[[3, 4]], negative=[], satisfy=True)\nclause(id=2, positive=[[4, 5]], negative=[], satisfy=False)\nRoot: ([] -> 3 ([] -> 4 ([5])) -> 4 ([5]))\nState: [([5]), ([5]), ]\nMet_phrases: None\n{3, 4, 5}\n(1,)\n(0, 2)\n\nclause(id=0, positive=[[3, 4, 5]], negative=[], satisfy=True)\nclause(id=1, positive=[[3, 4]], negative=[], satisfy=True)\nclause(id=2, positive=[[4, 5]], negative=[], satisfy=True)\nRoot: ([])\nState: [([]), ]\nMet_phrases: None\nset()\n(0, 1, 2)\n()\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# top k\nNEGATIVE_INF = -1000\n\ndef topk_huggingface(timestep: int,\n                     batch_size: int,\n                     beam_size: int,\n                     vocab_size: int,\n                     pad_token_id: int,\n                     prune_factor: int,\n                     sat_tolerance: int,\n                     beta: float,\n                     inactive: np.array,\n                     scores: np.array,\n                     hypotheses: List[ConstrainedHypothesis],\n                     num_fill: int,\n                     early_stop: float = None) -> Tuple[np.array, np.array,\n                                                        List[List[Union[ConstrainedHypothesis, None]]],\n                                                        List[List[int]]]:\n    \"\"\"\n    Builds a new topk list such that the beam contains hypotheses having completed different numbers of constraints.\n    These items are built from three different types: (1) the best items across the whole\n    scores matrix, (2) the set of words that must follow existing constraints, and (3) k-best items from each row.\n\n    :param batch_size: The number of segments in the batch.\n    :param beam_size: The length of the beam for each segment.\n    :param vocab_size: The size of vocabulary.\n    :param pad_token_id:\n    :param lambda_1:\n    :param sat_tolerance:\n    :param inactive: Array listing inactive rows (shape: (batch_size, beam_size,)).\n    :param scores: The scores array (shape: (batch_size, beam_size * target_vocab_size)).\n    :param hypotheses: The list of hypothesis objects. (length: (batch_size * beam_size,))\n    :param num_mets: The list of int how many constraints satisfied. (length: (batch_size * beam_size,))\n    :param num_fill: The number of required return beam\n    :return: A tuple containing the best hypothesis rows, the best hypothesis words, the scores,\n        the updated constrained hypotheses, and the updated set of inactive hypotheses.\n    \"\"\"\n\n    seq_scores, raw_token_idx = torch.topk(scores, beam_size, dim=1, largest=True, sorted=True)\n    best_ids = (raw_token_idx // vocab_size).cpu().numpy()\n    best_word_ids = (raw_token_idx % vocab_size).cpu().numpy()\n    seq_scores = seq_scores.cpu().detach().numpy()\n\n    scores = torch.reshape(scores, [batch_size, beam_size, -1]).cpu().detach().numpy()\n\n    select_best_ids = np.ones((batch_size, num_fill)) * -1\n    select_best_word_ids = np.ones((batch_size, num_fill)) * -1\n    select_seq_scores = np.zeros((batch_size, num_fill))\n    select_hypotheses = [[None] * num_fill for _ in range(batch_size)]\n    select_num_mets = [[-1] * num_fill for _ in range(batch_size)]\n\n    for sentno in range(batch_size):\n        rows = slice(sentno * beam_size, sentno * beam_size + beam_size)\n        if all([x is None for x in hypotheses[rows]]):\n            select_best_ids[sentno] = [0] * num_fill\n            select_best_word_ids[sentno] = [pad_token_id] * num_fill\n            select_seq_scores[sentno] = [0] * num_fill\n            select_hypotheses[sentno] = [None] * num_fill\n            select_num_mets[sentno] = [-1] * num_fill\n            continue\n\n        assert not any([x is None for x in hypotheses[rows]]), 'Bad state'\n\n        select_best_ids[sentno], select_best_word_ids[sentno], select_seq_scores[sentno],\\\n            select_hypotheses[sentno], select_num_mets[sentno] = _sequential_topk(timestep,\n                                                                                  beam_size,\n                                                                                  prune_factor,\n                                                                                  sat_tolerance,\n                                                                                  beta,\n                                                                                  inactive[sentno],\n                                                                                  scores[sentno],\n                                                                                  hypotheses[rows],\n                                                                                  best_ids[sentno],\n                                                                                  best_word_ids[sentno],\n                                                                                  seq_scores[sentno],\n                                                                                  num_fill=num_fill,\n                                                                                  early_stop=early_stop)\n\n    select_raw_token_idx = select_best_ids * vocab_size + select_best_word_ids\n    return select_seq_scores, select_raw_token_idx, select_hypotheses, select_num_mets\n\n\ndef _sequential_topk(timestep: int,\n                     beam_size: int,\n                     prune_factor: int,\n                     sat_tolerance: int,\n                     beta: float,\n                     inactive: np.array,\n                     scores: np.array,\n                     hypotheses: List[ConstrainedHypothesis],\n                     best_ids: np.array,\n                     best_word_ids: np.array,\n                     sequence_scores: np.array,\n                     num_fill: int = None,\n                     early_stop: float = None) -> Tuple[np.array, np.array, np.array,\n                                                        List[ConstrainedHypothesis], List[int]]:\n    \"\"\"\n    Builds a new topk list such that the beam contains hypotheses having completed different numbers of constraints.\n    These items are built from three different types: (1) the best items across the whole\n    scores matrix, (2) the set of words that must follow existing constraints, and (3) k-best items from each row.\n\n    :param timestep: The current decoder timestep.\n    :param beam_size: The length of the beam for each segment.\n    :param inactive: Array listing inactive rows (shape: (beam_size,)).\n    :param scores: The scores array (shape: (beam_size, target_vocab_size)).\n    :param hypotheses: The list of hypothesis objects. (length: (beam_size,))\n    :param best_ids: The current list of best hypotheses (shape: (beam_size,)).\n    :param best_word_ids: The parallel list of best word IDs (shape: (beam_size,)).\n    :param sequence_scores: (shape: (beam_size, 1)).\n    :return: A tuple containing the best hypothesis rows, the best hypothesis words, the scores,\n        the updated constrained hypotheses, and the updated set of inactive hypotheses.\n    \"\"\"\n\n    candidates = set()\n    finished_candidates = set()\n    # the best item (constrained or not) in that row\n    best_next = np.argmax(scores, axis=1)\n    rank = rankdata(-1 * scores, method='dense').reshape(scores.shape)\n\n    # (1) Add all of the top-k items (which were passed) in as long as they pass the constraints\n    for row, col, seq_score in zip(best_ids, best_word_ids, sequence_scores):\n        row, col = int(row), int(col)\n        seq_score = float(seq_score)\n        new_item = hypotheses[row].advance(col)\n        cand = ConstrainedCandidate(row, col, seq_score, new_item)\n        if cand.hypothesis.finished():\n            finished_candidates.add(cand)\n        elif hypotheses[row].is_valid(col) or int(best_next[row]) == col:\n            candidates.add(cand)\n\n    hit = np.stack([best_ids, best_word_ids], axis=1).tolist()\n    # For each hypothesis, we add (2) all the constraints that could follow it and\n    # (3) the best item (constrained or not) in that row\n    for row in range(beam_size):\n        if inactive[row] or hypotheses[row]:\n            continue\n\n        hyp = hypotheses[row]\n        \n        if hyp.positive_state is None:\n            inactive[row] = True\n            continue\n        print(f\"positive state:{hyp.positive_state.allowed()}\")\n\n        # (2) add all the constraints that could extend this\n        nextones = hyp.positive_state.allowed()\n\n        # (3) add the best items (if it's valid)\n        best_k = np.argsort(scores[row])[::-1][:beam_size]\n        for col in best_k:\n            if hyp.is_valid(col):\n                nextones.add(col)\n\n        # Now, create new candidates for each of these items\n        for col in nextones:\n            if [row, col] not in hit and (rank[row, col] < prune_factor and scores[row, col] > NEGATIVE_INF):\n                new_item = hyp.advance(col)\n                score = scores[row, col]\n                cand = ConstrainedCandidate(row, col, score, new_item)\n                if cand.hypothesis.finished() and col in cand.hypothesis.eos():\n                    finished_candidates.add(cand)\n                else:\n                    candidates.add(cand)\n\n        # Add finished candidates in finished set:\n        if hyp.finished():\n            best_k = np.argsort(scores[row])[::-1][:int(beam_size * early_stop)]\n            for col in best_k:\n                if col in hyp.eos() and scores[row, col] > NEGATIVE_INF:\n                    new_item = hyp.advance(col)\n                    score = scores[row, col]\n                    cand = ConstrainedCandidate(row, col, score, new_item)\n                    finished_candidates.add(cand)\n\n    if num_fill is not None:\n        assert num_fill > beam_size, \"at least select number of beam candidates\"\n    else:\n        num_fill = beam_size\n\n    # all the sentences finish without satisfy all constraints\n    if (not candidates) and (not finished_candidates):\n        print('edge case')\n        for row, col, seq_score in zip(best_ids, best_word_ids, sequence_scores):\n            row, col = int(row), int(col)\n            seq_score = float(seq_score)\n            new_item = hypotheses[row].advance(col)\n            cand = ConstrainedCandidate(row, col, seq_score, new_item)\n            candidates.add(cand)\n\n    chunk_candidates = []\n    if candidates:\n        # Sort the candidates.\n        sorted_candidates = sorted(candidates, key=attrgetter('score'), reverse=True)\n        max_satisfy = max([x.hypothesis.num_met() for x in sorted_candidates])\n        sorted_candidates = [x for x in sorted_candidates if x.hypothesis.num_met() >= max_satisfy - sat_tolerance]\n\n        for cand in sorted_candidates:\n            cand.rank = cand.score / (timestep + 1)\n            if cand.hypothesis.max_process:\n                cand.rank -= beta * math.log(cand.hypothesis.max_process)\n        sorted_candidates = sorted(sorted_candidates, key=attrgetter('rank'), reverse=True)\n\n        # Bucket candidates in each group by met order\n        all_orders = set([x.hypothesis.met_order() for x in sorted_candidates])\n        grouped_order_candidates = [[x for x in sorted_candidates if x.hypothesis.met_order() == o] for o in all_orders]\n\n        # Group the top_i candidate of each group in chunk\n        chunk_candidates = []\n        num_chunk = max([len(x) for x in grouped_order_candidates])\n        for i in range(num_chunk):\n            chunk_i = []\n            for g in grouped_order_candidates:\n                if len(g) > i:\n                    chunk_i.append(g[i])\n            chunk_candidates.append(chunk_i)\n        # Sort candidates in each chunk by score\n        chunk_candidates = [sorted(x, key=attrgetter('rank'), reverse=True) for x in chunk_candidates]\n\n    pruned_candidates = sorted(finished_candidates, key=attrgetter('score'), reverse=True)[:(num_fill if not candidates else beam_size)]\n    num_finish = len(pruned_candidates)\n    for chunk in chunk_candidates:\n        if len(pruned_candidates) >= num_fill:\n            break\n\n        chunk = [x for x in chunk if x not in pruned_candidates]\n        if not chunk:\n            continue\n\n        pruned_candidates.extend(chunk[:num_fill - len(pruned_candidates)])\n\n    if num_fill > beam_size:\n        if candidates:\n            select_num = num_finish + beam_size\n            complete_candidates = sorted(pruned_candidates[:num_finish], key=attrgetter('score'), reverse=True)\n            include_candidates = sorted(pruned_candidates[num_finish:select_num], key=attrgetter('score'), reverse=True)\n            extra_candidates = sorted(pruned_candidates[select_num:], key=attrgetter('score'), reverse=True)\n            pruned_candidates = complete_candidates + include_candidates + extra_candidates\n    else:\n        pruned_candidates = sorted(pruned_candidates, key=attrgetter('score'), reverse=True)\n\n    num_pruned_candidates = len(pruned_candidates)\n\n    inactive = np.zeros(num_fill)\n    inactive[:num_pruned_candidates] = 0\n\n    # Pad the beam so array assignment still works\n    if num_pruned_candidates < num_fill:\n        inactive[num_pruned_candidates:] = 1\n        pruned_candidates += [pruned_candidates[num_pruned_candidates - 1]] * (num_fill - num_pruned_candidates)\n\n    assert len(pruned_candidates) == num_fill, 'candidates number mismatch'\n\n    return (np.array([x.row for x in pruned_candidates]),\n            np.array([x.col for x in pruned_candidates]),\n            np.array([x.score for x in pruned_candidates]),\n            [x.hypothesis for x in pruned_candidates],\n            [x.hypothesis.num_met() for x in pruned_candidates])\n","metadata":{"execution":{"iopub.status.busy":"2023-05-31T21:05:30.276767Z","iopub.execute_input":"2023-05-31T21:05:30.277113Z","iopub.status.idle":"2023-05-31T21:05:30.322944Z","shell.execute_reply.started":"2023-05-31T21:05:30.277078Z","shell.execute_reply":"2023-05-31T21:05:30.321993Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# generate\nlogger = logging.getLogger(__name__)\n\n\n@torch.no_grad()\ndef postprocess_next_token_scores(\n        self,\n        scores,\n        input_ids,\n        no_repeat_ngram_size,\n        bad_words_ids,\n        cur_len,\n        min_length,\n        max_length,\n        eos_token_id,\n        repetition_penalty,\n        batch_size,\n        num_beams,\n):\n    # repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)\n    if repetition_penalty != 1.0:\n        self.enforce_repetition_penalty_(\n            scores, batch_size, num_beams, input_ids, repetition_penalty,\n        )\n\n    # set eos token prob to zero if min_length is not reached\n    if eos_token_id is not None and cur_len < min_length:\n        scores[:, eos_token_id] = -float(\"inf\")\n\n    if no_repeat_ngram_size > 0:\n        # calculate a list of banned tokens to prevent repetitively generating the same ngrams\n        num_batch_hypotheses = batch_size * num_beams\n        # from fairseq: https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345\n        banned_batch_tokens = calc_banned_ngram_tokens(\n            input_ids, num_batch_hypotheses, no_repeat_ngram_size, cur_len\n        )\n        for i, banned_tokens in enumerate(banned_batch_tokens):\n            scores[i, banned_tokens] = -float(\"inf\")\n\n    if bad_words_ids is not None:\n        # calculate a list of banned tokens according to bad words\n        banned_tokens = calc_banned_bad_words_ids(input_ids, bad_words_ids)\n\n        for i, banned_tokens in enumerate(banned_tokens):\n            scores[i, banned_tokens] = -float(\"inf\")\n\n    return scores\ndef generate(\n    self,\n    input_ids: Optional[torch.LongTensor] = None,\n    max_length: Optional[int] = None,\n    min_length: Optional[int] = None,\n    do_sample: Optional[bool] = None,\n    early_stopping: Optional[bool] = None,\n    num_beams: Optional[int] = None,\n    temperature: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    repetition_penalty: Optional[float] = None,\n    bad_words_ids: Optional[Iterable[int]] = None,\n    bos_token_id: Optional[int] = None,\n    pad_token_id: Optional[int] = None,\n    eos_token_id: Optional[int] = None,\n    length_penalty: Optional[float] = None,\n    no_repeat_ngram_size: Optional[int] = None,\n    num_return_sequences: Optional[int] = None,\n    attention_mask: Optional[torch.LongTensor] = None,\n    decoder_start_token_id: Optional[int] = None,\n    use_cache: Optional[bool] = None,\n    constraints: Optional[List[Optional[ConstrainedHypothesis]]] = None,\n    prune_factor: Optional[int] = None,\n    sat_tolerance: Optional[int] = None,\n    beta: Optional[int] = None,\n    early_stop: Optional[float] = None,\n    **model_specific_kwargs\n) -> torch.LongTensor:\n    r\"\"\" Generates sequences for models with a LM head. The method currently supports greedy decoding, beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.\n\n    Adapted in part from `Facebook's XLM beam search code`_.\n\n    .. _`Facebook's XLM beam search code`:\n       https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529\n\n\n    Parameters:\n\n        input_ids: (`optional`) `torch.LongTensor` of shape `(batch_size, sequence_length)`\n            The sequence used as a prompt for the generation. If `None` the method initializes\n            it as an empty `torch.LongTensor` of shape `(1,)`.\n\n        max_length: (`optional`) int\n            The max length of the sequence to be generated.  Between `min_length` and infinity. Default to 20.\n\n        min_length: (`optional`) int\n            The min length of the sequence to be generated.  Between 0 and infinity. Default to 0.\n\n        do_sample: (`optional`) bool\n            If set to `False` greedy decoding is used. Otherwise sampling is used. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.\n\n        early_stopping: (`optional`) bool\n            if set to `True` beam search is stopped when at least `num_beams` sentences finished per batch. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.\n\n        num_beams: (`optional`) int\n            Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1.\n\n        temperature: (`optional`) float\n            The value used to module the next token probabilities. Must be strictly positive. Default to 1.0.\n\n        top_k: (`optional`) int\n            The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Default to 50.\n\n        top_p: (`optional`) float\n            The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Must be between 0 and 1. Default to 1.\n\n        repetition_penalty: (`optional`) float\n            The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.\n\n        pad_token_id: (`optional`) int\n            Padding token. Default to specicic model pad_token_id or None if it does not exist.\n\n        bos_token_id: (`optional`) int\n            BOS token. Defaults to `bos_token_id` as defined in the models config.\n\n        eos_token_id: (`optional`) int\n            EOS token. Defaults to `eos_token_id` as defined in the models config.\n\n        length_penalty: (`optional`) float\n            Exponential penalty to the length. Default to 1.\n\n        no_repeat_ngram_size: (`optional`) int\n            If set to int > 0, all ngrams of size `no_repeat_ngram_size` can only occur once.\n        bad_words_ids: (`optional`) list of lists of int\n            `bad_words_ids` contains tokens that are not allowed to be generated. In order to get the tokens of the words that should not appear in the generated text, use `tokenizer.encode(bad_word, add_prefix_space=True)`.\n\n        num_return_sequences: (`optional`) int\n            The number of independently computed returned sequences for each element in the batch. Default to 1.\n\n        attention_mask (`optional`) obj: `torch.LongTensor` of same shape as `input_ids`\n            Mask to avoid performing attention on padding token indices.\n            Mask values selected in ``[0, 1]``:\n            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n            Defaults to `None`.\n\n            `What are attention masks? <../glossary.html#attention-mask>`__\n\n        decoder_start_token_id=None: (`optional`) int\n            If an encoder-decoder model starts decoding with a different token than BOS.\n            Defaults to `None` and is changed to `BOS` later.\n\n        use_cache: (`optional`) bool\n            If `use_cache` is True, past key values are used to speed up decoding if applicable to model. Defaults to `True`.\n\n        model_specific_kwargs: (`optional`) dict\n            Additional model specific kwargs will be forwarded to the `forward` function of the model.\n\n    Return:\n\n        output: `torch.LongTensor` of shape `(batch_size * num_return_sequences, sequence_length)`\n            sequence_length is either equal to max_length or shorter if all batches finished early due to the `eos_token_id`\n\n    \"\"\"\n\n    # We cannot generate if the model does not have a LM head\n    if self.get_output_embeddings() is None:\n        raise AttributeError(\n            \"You tried to generate sequences with a model that does not have a LM Head.\"\n            \"Please use another model class (e.g. `OpenAIGPTLMHeadModel`, `XLNetLMHeadModel`, `GPT2LMHeadModel`, `CTRLLMHeadModel`, `T5WithLMHeadModel`, `TransfoXLLMHeadModel`, `XLMWithLMHeadModel`, `BartForConditionalGeneration` )\"\n        )\n\n    max_length = max_length if max_length is not None else self.config.max_length\n    min_length = min_length if min_length is not None else self.config.min_length\n    do_sample = do_sample if do_sample is not None else self.config.do_sample\n    early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping\n    use_cache = use_cache if use_cache is not None else False # self.config.use_cache\n    num_beams = num_beams if num_beams is not None else self.config.num_beams\n    temperature = temperature if temperature is not None else self.config.temperature\n    top_k = top_k if top_k is not None else self.config.top_k\n    top_p = top_p if top_p is not None else self.config.top_p\n    repetition_penalty = repetition_penalty if repetition_penalty is not None else self.config.repetition_penalty\n    bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id\n    pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\n    eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\n    length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\n    no_repeat_ngram_size = (\n        no_repeat_ngram_size if no_repeat_ngram_size is not None else self.config.no_repeat_ngram_size\n    )\n    bad_words_ids = bad_words_ids if bad_words_ids is not None else self.config.bad_words_ids\n    num_return_sequences = (\n        num_return_sequences if num_return_sequences is not None else self.config.num_return_sequences\n    )\n    decoder_start_token_id = (\n        decoder_start_token_id if decoder_start_token_id is not None else self.config.decoder_start_token_id\n    )\n\n    if input_ids is not None:\n        batch_size = input_ids.shape[0]  # overriden by the input batch_size\n    else:\n        batch_size = 1\n\n    assert isinstance(max_length, int) and max_length > 0, \"`max_length` should be a strictly positive integer.\"\n    assert isinstance(min_length, int) and min_length >= 0, \"`min_length` should be a positive integer.\"\n    assert isinstance(do_sample, bool), \"`do_sample` should be a boolean.\"\n    assert isinstance(early_stopping, bool), \"`early_stopping` should be a boolean.\"\n    assert isinstance(use_cache, bool), \"`use_cache` should be a boolean.\"\n    assert isinstance(num_beams, int) and num_beams > 0, \"`num_beams` should be a strictly positive integer.\"\n    assert temperature > 0, \"`temperature` should be strictly positive.\"\n    assert isinstance(top_k, int) and top_k >= 0, \"`top_k` should be a positive integer.\"\n    assert 0 <= top_p <= 1, \"`top_p` should be between 0 and 1.\"\n    assert repetition_penalty >= 1.0, \"`repetition_penalty` should be >= 1.\"\n    assert input_ids is not None or (\n        isinstance(bos_token_id, int) and bos_token_id >= 0\n    ), \"If input_ids is not defined, `bos_token_id` should be a positive integer.\"\n    assert pad_token_id is None or (\n        isinstance(pad_token_id, int) and (pad_token_id >= 0)\n    ), \"`pad_token_id` should be a positive integer.\"\n    assert (eos_token_id is None) or (\n        isinstance(eos_token_id, int) and (eos_token_id >= 0)\n    ), \"`eos_token_id` should be a positive integer.\"\n    assert length_penalty > 0, \"`length_penalty` should be strictly positive.\"\n    assert (\n        isinstance(no_repeat_ngram_size, int) and no_repeat_ngram_size >= 0\n    ), \"`no_repeat_ngram_size` should be a positive integer.\"\n    assert (\n        isinstance(num_return_sequences, int) and num_return_sequences > 0\n    ), \"`num_return_sequences` should be a strictly positive integer.\"\n    assert (\n        bad_words_ids is None or isinstance(bad_words_ids, list) and isinstance(bad_words_ids[0], list)\n    ), \"`bad_words_ids` is either `None` or a list of lists of tokens that should not be generated\"\n\n    if input_ids is None:\n        assert isinstance(bos_token_id, int) and bos_token_id >= 0, (\n            \"you should either supply a context to complete as `input_ids` input \"\n            \"or a `bos_token_id` (integer >= 0) as a first token to start the generation.\"\n        )\n        input_ids = torch.full(\n            (batch_size, 1), bos_token_id, dtype=torch.long, device=next(self.parameters()).device,\n        )\n    else:\n        assert input_ids.dim() == 2, \"Input prompt should be of shape (batch_size, sequence length).\"\n\n    # not allow to duplicate outputs when greedy decoding\n    if do_sample is False:\n        if num_beams == 1:\n            # no_beam_search greedy generation conditions\n            assert (\n                num_return_sequences == 1\n            ), \"Greedy decoding will always produce the same output for num_beams == 1 and num_return_sequences > 1. Please set num_return_sequences = 1\"\n\n        else:\n            # beam_search greedy generation conditions\n            assert (\n                num_beams >= num_return_sequences\n            ), \"Greedy beam search decoding cannot return more sequences than it has beams. Please set num_beams >= num_return_sequences\"\n\n    # create attention mask if necessary\n    if (attention_mask is None) and (pad_token_id is not None) and (pad_token_id in input_ids):\n        attention_mask = input_ids.ne(pad_token_id).long()\n    elif attention_mask is None:\n        attention_mask = input_ids.new_ones(input_ids.shape)\n\n    # set pad_token_id to eos_token_id if not set. Important that this is done after\n    # attention_mask is created\n    if pad_token_id is None and eos_token_id is not None:\n        logger.warning(\n            \"Setting `pad_token_id` to {} (first `eos_token_id`) to generate sequence\".format(eos_token_id)\n        )\n        pad_token_id = eos_token_id\n\n    # current position and vocab size\n    if hasattr(self.config, \"vocab_size\"):\n        vocab_size = self.config.vocab_size\n    elif (\n        self.config.is_encoder_decoder\n        and hasattr(self.config, \"decoder\")\n        and hasattr(self.config.decoder, \"vocab_size\")\n    ):\n        vocab_size = self.config.decoder.vocab_size\n\n    # set effective batch size and effective batch multiplier according to do_sample\n    if do_sample:\n        effective_batch_size = batch_size * num_return_sequences\n        effective_batch_mult = num_return_sequences\n    else:\n        effective_batch_size = batch_size\n        effective_batch_mult = 1\n\n    if self.config.is_encoder_decoder:\n        if decoder_start_token_id is None:\n            decoder_start_token_id = bos_token_id\n\n        assert (\n            decoder_start_token_id is not None\n        ), \"decoder_start_token_id or bos_token_id has to be defined for encoder-decoder generation\"\n        assert hasattr(self, \"get_encoder\"), \"{} should have a 'get_encoder' function defined\".format(self)\n        assert callable(self.get_encoder), \"{} should be a method\".format(self.get_encoder)\n\n        # get encoder and store encoder outputs\n        encoder = self.get_encoder()\n\n        encoder_outputs: tuple = encoder(input_ids, attention_mask=attention_mask)\n\n    # Expand input ids if num_beams > 1 or num_return_sequences > 1\n    if num_return_sequences > 1 or num_beams > 1:\n        input_ids_len = input_ids.shape[-1]\n        input_ids = input_ids.unsqueeze(1).expand(batch_size, effective_batch_mult * num_beams, input_ids_len)\n        attention_mask = attention_mask.unsqueeze(1).expand(\n            batch_size, effective_batch_mult * num_beams, input_ids_len\n        )\n\n        input_ids = input_ids.contiguous().view(\n            effective_batch_size * num_beams, input_ids_len\n        )  # shape: (batch_size * num_return_sequences * num_beams, cur_len)\n        attention_mask = attention_mask.contiguous().view(\n            effective_batch_size * num_beams, input_ids_len\n        )  # shape: (batch_size * num_return_sequences * num_beams, cur_len)\n\n    if self.config.is_encoder_decoder:\n        # create empty decoder_input_ids\n        input_ids = torch.full(\n            (effective_batch_size * num_beams, 1),\n            decoder_start_token_id,\n            dtype=torch.long,\n            device=next(self.parameters()).device,\n        )\n        cur_len = 1\n\n        assert (\n            batch_size == encoder_outputs[0].shape[0]\n        ), f\"expected encoder_outputs[0] to have 1st dimension bs={batch_size}, got {encoder_outputs[0].shape[0]} \"\n\n        # expand batch_idx to assign correct encoder output for expanded input_ids (due to num_beams > 1 and num_return_sequences > 1)\n        expanded_batch_idxs = (\n            torch.arange(batch_size)\n            .view(-1, 1)\n            .repeat(1, num_beams * effective_batch_mult)\n            .view(-1)\n            .to(input_ids.device)\n        )\n        # expand encoder_outputs\n        encoder_outputs = (encoder_outputs[0].index_select(0, expanded_batch_idxs), *encoder_outputs[1:])\n\n    else:\n        encoder_outputs = None\n        cur_len = input_ids.shape[-1]\n    free_gpu_cache()\n    if num_beams > 1:\n        output = _generate_beam_search(\n            self,\n            input_ids=input_ids,\n            cur_len=cur_len,\n            max_length=max_length,\n            min_length=min_length,\n            do_sample=do_sample,\n            early_stopping=early_stopping,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            repetition_penalty=repetition_penalty,\n            no_repeat_ngram_size=no_repeat_ngram_size,\n            bad_words_ids=bad_words_ids,\n            bos_token_id=bos_token_id,\n            pad_token_id=pad_token_id,\n            decoder_start_token_id=decoder_start_token_id,\n            eos_token_id=eos_token_id,\n            batch_size=effective_batch_size,\n            num_return_sequences=num_return_sequences,\n            length_penalty=length_penalty,\n            num_beams=num_beams,\n            vocab_size=vocab_size,\n            encoder_outputs=encoder_outputs,\n            attention_mask=attention_mask,\n            use_cache=use_cache,\n            constraints=constraints,\n            prune_factor=prune_factor,\n            sat_tolerance=sat_tolerance,\n            beta=beta,\n            early_stop=early_stop,\n            model_specific_kwargs=model_specific_kwargs,\n        )\n    else:\n        raise NotImplementedError\n    return output\n\n\nclass BeamHypotheses(object):\n    def __init__(self, num_beams, max_length, length_penalty, early_stopping):\n        \"\"\"\n        Initialize n-best list of hypotheses.\n        \"\"\"\n        self.max_length = max_length - 1  # ignoring bos_token\n        self.length_penalty = length_penalty\n        self.early_stopping = early_stopping\n        self.num_beams = num_beams * 2\n        self.beams = []\n        self.worst_score = 1e9\n\n    def __len__(self):\n        \"\"\"\n        Number of hypotheses in the list.\n        \"\"\"\n        return len(self.beams)\n\n    def add(self, hyp, sum_logprobs, num_met):\n        \"\"\"\n        Add a new hypothesis to the list.\n        \"\"\"\n        score = sum_logprobs / len(hyp) ** self.length_penalty\n        #score = sum_logprobs / math.pow((5 + len(hyp) + 1) / 6.0, self.length_penalty)\n        if len(self) < self.num_beams or score > self.worst_score:\n            self.beams.append((score, hyp, num_met))\n            if len(self) > self.num_beams:\n                sorted_scores = sorted([(s, idx) for idx, (s, _, _) in enumerate(self.beams)])\n                del self.beams[sorted_scores[0][1]]\n                self.worst_score = sorted_scores[1][0]\n            else:\n                self.worst_score = min(score, self.worst_score)\n\n    def is_done(self, best_sum_logprobs, cur_len=None):\n        \"\"\"\n        If there are enough hypotheses and that none of the hypotheses being generated\n        can become better than the worst one in the heap, then we are done with this sentence.\n        \"\"\"\n\n        if len(self) < self.num_beams:\n            return False\n        elif self.early_stopping:\n            return True\n        else:\n            if cur_len is None:\n                cur_len = self.max_length\n            cur_score = best_sum_logprobs / cur_len ** self.length_penalty\n            #cur_score = best_sum_logprobs / math.pow((5 + cur_len + 1) / 6.0, self.length_penalty)\n            ret = self.worst_score >= cur_score\n            return ret\n\n\ndef _generate_beam_search(\n        self,\n        input_ids,\n        cur_len,\n        max_length,\n        min_length,\n        do_sample,\n        early_stopping,\n        temperature,\n        top_k,\n        top_p,\n        repetition_penalty,\n        no_repeat_ngram_size,\n        bad_words_ids,\n        bos_token_id,\n        pad_token_id,\n        eos_token_id,\n        decoder_start_token_id,\n        batch_size,\n        num_return_sequences,\n        length_penalty,\n        num_beams,\n        vocab_size,\n        encoder_outputs,\n        attention_mask,\n        use_cache,\n        constraints,\n        prune_factor,\n        sat_tolerance,\n        beta,\n        early_stop,\n        model_specific_kwargs,\n):\n    \"\"\" Generate sequences for each example with beam search.\n    \"\"\"\n    # end condition\n    cons_eos = constraints[0].eos()\n\n    last_non_masked_idx = (torch.sum(attention_mask, dim=1) - 1).int()\n    start_idx = (last_non_masked_idx).view(-1, 1).repeat(1, self.config.vocab_size).unsqueeze(1).long()\n\n    init_length = cur_len\n    position_ids = torch.tensor([list(range(init_length)) for i in range(input_ids.shape[0])])\n    for i, position_ids_slice in enumerate(position_ids):\n        position_ids_slice[last_non_masked_idx[i]:] = position_ids_slice[last_non_masked_idx[i]]\n    position_ids = position_ids.to(input_ids.device)\n\n    # generated hypotheses\n    generated_hyps = [\n        BeamHypotheses(num_beams, max_length, length_penalty, early_stopping=early_stopping)\n        for _ in range(batch_size)\n    ]\n\n    # scores for each sentence in the beam\n    beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n\n    # for greedy decoding it is made sure that only tokens of the first beam are considered to avoid sampling the exact same tokens three times\n    if do_sample is False:\n        beam_scores[:, 1:] = -1e9\n    beam_scores = beam_scores.view(-1)  # shape (batch_size * num_beams,)\n\n    # cache compute states\\\n    past = (encoder_outputs, None) if encoder_outputs is not None else None\n\n    # done sentences\n    done = [False for _ in range(batch_size)]\n\n    # init number of met clauses\n    num_mets = [x.num_met() for x in constraints]\n\n    while cur_len < max_length:\n        model_inputs = self.prepare_inputs_for_generation(\n            input_ids, past=past, attention_mask=attention_mask, use_cache=use_cache, **model_specific_kwargs\n        )\n        model_inputs[\"attention_mask\"] = attention_mask\n        model_inputs[\"position_ids\"] = position_ids[:, -1].unsqueeze(-1) if past else position_ids\n\n        outputs = self(**model_inputs)  # (batch_size * num_beams, cur_len, vocab_size)\n        if cur_len == init_length:\n            next_token_logits = outputs[0].gather(1, start_idx).squeeze(1)\n        else:\n            next_token_logits = outputs[0][:, -1, :]  # (batch_size * num_beams, vocab_size)\n\n        # if model has past, then set the past variable to speed up decoding\n        # if self._use_cache(outputs, use_cache):\n        #     past = outputs[1]\n\n        # repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)\n        if repetition_penalty != 1.0:\n            self.enforce_repetition_penalty_(\n                next_token_logits, batch_size, num_beams, input_ids, repetition_penalty,\n            )\n\n        if temperature != 1.0:\n            next_token_logits = next_token_logits / temperature\n\n        if self.config.is_encoder_decoder and do_sample is False:\n            next_token_logits = self.adjust_logits_during_generation(\n                next_token_logits, cur_len=cur_len, max_length=max_length\n            )\n\n        scores = F.log_softmax(next_token_logits, dim=-1)  # (batch_size * num_beams, vocab_size)\n#         free_gpu_cache()\n        scores = postprocess_next_token_scores(\n            self=self,\n            scores=scores,\n            input_ids=input_ids,\n            no_repeat_ngram_size=no_repeat_ngram_size,\n            bad_words_ids=bad_words_ids,\n            cur_len=cur_len,\n            min_length=min_length,\n            max_length=max_length,\n            eos_token_id=eos_token_id,\n            repetition_penalty=repetition_penalty,\n            batch_size=batch_size,\n            num_beams=num_beams,\n        )\n\n        assert scores.shape == (batch_size * num_beams, vocab_size), \"Shapes of scores: {} != {}\".format(\n            scores.shape, (batch_size * num_beams, vocab_size)\n        )\n\n        avoid_idx = []\n        for i, c in enumerate(constraints):\n            if c is not None:\n                avoid_idx.extend([[i, x] for x in c.avoid()])\n                if cur_len - init_length < min_length:\n                    avoid_idx.extend([[i, x] for x in c.eos()])\n        if avoid_idx:\n            banned_mask = torch.LongTensor(avoid_idx)\n            indices = torch.ones(len(banned_mask))\n            banned_mask = torch.sparse.LongTensor(banned_mask.t(), indices, scores.size()).to(\n                scores.device).to_dense().bool()\n            scores.masked_fill_(banned_mask, -float(\"inf\"))\n\n        if do_sample:\n            raise NotImplementedError\n        else:\n            next_scores = scores + beam_scores[:, None].expand_as(scores)  # (batch_size * num_beams, vocab_size)\n\n            # re-organize to group the beam together (we are keeping top hypothesis accross beams)\n            full_scores = next_scores.view(\n                batch_size, num_beams * vocab_size\n            )  # (batch_size, num_beams * vocab_size)\n\n            next_scores, next_tokens = torch.topk(full_scores, 2 * num_beams, dim=1, largest=True, sorted=True)\n\n            pick_scores, pick_tokens, constraints, num_mets = topk_huggingface(timestep=cur_len,\n                                                                               batch_size=batch_size,\n                                                                               beam_size=num_beams,\n                                                                               vocab_size=vocab_size,\n                                                                               pad_token_id=pad_token_id,\n                                                                               prune_factor=prune_factor,\n                                                                               sat_tolerance=sat_tolerance,\n                                                                               beta=beta,\n                                                                               inactive=np.zeros((batch_size, num_beams)),\n                                                                               scores=full_scores,\n                                                                               hypotheses=constraints,\n                                                                               num_fill=2 * num_beams,\n                                                                               early_stop=early_stop)\n\n            next_scores = torch.tensor(pick_scores, dtype=next_scores.dtype, device=next_scores.device)\n            next_tokens = torch.tensor(pick_tokens, dtype=next_tokens.dtype, device=next_tokens.device)\n\n        assert next_scores.size() == next_tokens.size() == (batch_size, 2 * num_beams)\n\n        # next batch beam content\n        next_batch_beam = []\n\n        # for each sentence\n        for batch_idx in range(batch_size):\n\n            # if we are done with this sentence, add a pad token\n            if done[batch_idx]:\n                assert (\n                    len(generated_hyps[batch_idx]) >= num_beams\n                ), \"Batch can only be done if at least {} beams have been generated\".format(num_beams)\n                assert (\n                    eos_token_id is not None and pad_token_id is not None\n                ), \"generated beams >= num_beams -> eos_token_id and pad_token have to be defined\"\n                next_batch_beam.extend([(0, pad_token_id, 0, None, -1)] * num_beams)  # pad the batch\n                continue\n\n            # next sentence beam content\n            next_sent_beam = []\n\n            # next tokens for this sentence\n            for beam_token_rank, (beam_token_id, beam_token_score, constraint, num_met) in enumerate(\n                zip(next_tokens[batch_idx], next_scores[batch_idx], constraints[batch_idx], num_mets[batch_idx])\n            ):\n                # get beam and token IDs\n                beam_id = beam_token_id // vocab_size\n                token_id = beam_token_id % vocab_size\n\n                effective_beam_id = batch_idx * num_beams + beam_id\n                sentence_end = token_id.item() in constraint.eos()\n                # add to generated hypotheses if end of sentence or last iteration\n                if ((eos_token_id is not None) and (token_id.item() == eos_token_id)) or sentence_end:\n                    # if beam_token does not belong to top num_beams tokens, it should not be added\n                    is_beam_token_worse_than_top_num_beams = beam_token_rank >= num_beams\n                    # if is_beam_token_worse_than_top_num_beams:\n                    #     continue\n                    generated_hyps[batch_idx].add(\n                        torch.cat((input_ids[effective_beam_id], token_id.view([1]))), beam_token_score.item(), num_met,\n                    )\n                else:\n                    # add next predicted token since it is not eos_token\n                    next_sent_beam.append((beam_token_score, token_id, effective_beam_id, constraint, num_met))\n\n                # once the beam for next step is full, don't add more tokens to it.\n                if len(next_sent_beam) == num_beams:\n                    break\n\n            # Check if were done so that we can save a pad step if all(done)\n            done[batch_idx] = done[batch_idx] or generated_hyps[batch_idx].is_done(\n                next_scores[batch_idx][:beam_token_rank + 1].max().item(), cur_len=cur_len\n            ) or not next_sent_beam\n\n            if len(next_sent_beam) < num_beams:\n                if next_sent_beam:\n                    pad_candidate = next_sent_beam[-1]\n                elif done[batch_idx]:\n                    pad_candidate = (0, pad_token_id, 0, None, -1)\n                else:\n                    raise ValueError('impossible search state')\n                next_sent_beam += [pad_candidate] * (num_beams - len(next_sent_beam))\n\n            # update next beam content\n            assert len(next_sent_beam) == num_beams, \"Beam should always be full\"\n            next_batch_beam.extend(next_sent_beam)\n            assert len(next_batch_beam) == num_beams * (batch_idx + 1), \"We should have added num_beams each step\"\n\n        # stop when we are done with each sentence\n        if all(done):\n            break\n\n        # sanity check / prepare next batch\n        assert len(next_batch_beam) == batch_size * num_beams\n        beam_scores = beam_scores.new([x[0] for x in next_batch_beam])\n        beam_tokens = input_ids.new([x[1] for x in next_batch_beam])\n        beam_idx = input_ids.new([x[2] for x in next_batch_beam])\n        constraints = [x[3] for x in next_batch_beam]\n        num_mets = [x[4] for x in next_batch_beam]\n\n        # re-order batch and update current length\n        input_ids = input_ids[beam_idx, :]\n        input_ids = torch.cat([input_ids, beam_tokens.unsqueeze(1)], dim=-1)\n        position_ids = position_ids[beam_idx, :]\n        position_ids = torch.cat([position_ids, (position_ids[:, -1] + 1).unsqueeze(-1)], dim=1)\n        cur_len = cur_len + 1\n\n        # re-order internal states\n        if past is not None:\n            past = self._reorder_cache(past, beam_idx)\n\n        # extend attention_mask for new generated input if only decoder\n        if self.config.is_encoder_decoder is False:\n            attention_mask = torch.cat(\n                [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n            )\n\n    # finalize all open beam hypotheses and add to generated hypotheses\n    for batch_idx in range(batch_size):\n        if done[batch_idx]:\n            continue\n\n        # test that beam scores match previously calculated scores if not eos and batch_idx not done\n        if eos_token_id is not None and all(\n            (token_id % vocab_size).item() not in cons_eos for token_id in next_tokens[batch_idx]\n        ):\n            assert torch.all(\n                next_scores[batch_idx, :num_beams] == beam_scores.view(batch_size, num_beams)[batch_idx]\n            ), \"If batch_idx is not done, final next scores: {} have to equal to accumulated beam_scores: {}\".format(\n                next_scores[:, :num_beams][batch_idx], beam_scores.view(batch_size, num_beams)[batch_idx],\n            )\n\n        # need to add best num_beams hypotheses to generated hyps\n        for beam_id in range(num_beams):\n            effective_beam_id = batch_idx * num_beams + beam_id\n            final_score = beam_scores[effective_beam_id].item()\n            final_tokens = input_ids[effective_beam_id]\n            final_num_met = num_mets[effective_beam_id]\n            generated_hyps[batch_idx].add(final_tokens, final_score, final_num_met)\n\n    # depending on whether greedy generation is wanted or not define different output_batch_size and output_num_return_sequences_per_batch\n    output_batch_size = batch_size if do_sample else batch_size * num_return_sequences\n    output_num_return_sequences_per_batch = 1 if do_sample else num_return_sequences\n\n    # select the best hypotheses\n    sent_lengths = input_ids.new(output_batch_size)\n    best, best_scores, best_sum_logprobs = [], [], []\n\n    # retrieve best hypotheses\n    for i, hypotheses in enumerate(generated_hyps):\n        sorted_hyps = sorted(hypotheses.beams, key=lambda x: (x[2], x[0]), reverse=True)\n        for j in range(output_num_return_sequences_per_batch):\n            effective_batch_idx = output_num_return_sequences_per_batch * i + j\n            best_score, best_hyp, _ = sorted_hyps[0]\n            sent_lengths[effective_batch_idx] = len(best_hyp)\n            best.append(best_hyp)\n            best_scores.append(best_score)\n            best_sum_logprobs.append(best_score * (len(best_hyp) ** length_penalty))\n\n    # shorter batches are padded\n    if sent_lengths.min().item() != sent_lengths.max().item():\n        assert pad_token_id is not None, \"`Pad_token_id` has to be defined\"\n        sent_max_len = min(sent_lengths.max().item() + 1, max_length)\n        decoded = input_ids.new(output_batch_size, sent_max_len).fill_(pad_token_id)\n\n        # fill with hypothesis and eos_token_id if necessary\n        for i, hypo in enumerate(best):\n            decoded[i, : sent_lengths[i]] = hypo\n            if sent_lengths[i] < max_length:\n                decoded[i, sent_lengths[i]] = eos_token_id\n    else:\n        # none of the hypotheses have an eos_token\n        assert (len(hypo) == max_length for hypo in best)\n        decoded = torch.stack(best).type(torch.long).to(next(self.parameters()).device)\n\n    return decoded, best_scores, best_sum_logprobs\n\n\ndef _reorder_cache(past: Tuple, beam_idx: Tensor) -> Tuple[Tensor]:\n    return tuple(layer_past.index_select(1, beam_idx) for layer_past in past)\n\n\ndef _use_cache(self, outputs, use_cache):\n    \"\"\"During generation, decide whether to pass the `past` variable to the next forward pass.\"\"\"\n    if len(outputs) <= 1 or use_cache is False:\n        return False\n    if hasattr(self.config, \"mem_len\") and self.config.mem_len == 0:\n        return False\n    return True\n\n\ndef calc_banned_ngram_tokens(prev_input_ids: Tensor, num_hypos: int, no_repeat_ngram_size: int, cur_len: int) -> None:\n    \"\"\"Copied from fairseq for no_repeat_ngram in beam_search\"\"\"\n    if cur_len + 1 < no_repeat_ngram_size:\n        # return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\n        return [[] for _ in range(num_hypos)]\n    generated_ngrams = [{} for _ in range(num_hypos)]\n    for idx in range(num_hypos):\n        gen_tokens = prev_input_ids[idx].tolist()\n        generated_ngram = generated_ngrams[idx]\n        for ngram in zip(*[gen_tokens[i:] for i in range(no_repeat_ngram_size)]):\n            prev_ngram_tuple = tuple(ngram[:-1])\n            generated_ngram[prev_ngram_tuple] = generated_ngram.get(prev_ngram_tuple, []) + [ngram[-1]]\n\n    def _get_generated_ngrams(hypo_idx):\n        # Before decoding the next token, prevent decoding of ngrams that have already appeared\n        start_idx = cur_len + 1 - no_repeat_ngram_size\n        ngram_idx = tuple(prev_input_ids[hypo_idx, start_idx:cur_len].tolist())\n        return generated_ngrams[hypo_idx].get(ngram_idx, [])\n\n    banned_tokens = [_get_generated_ngrams(hypo_idx) for hypo_idx in range(num_hypos)]\n    return banned_tokens\n\n\ndef calc_banned_bad_words_ids(prev_input_ids: Iterable[int], bad_words_ids: Iterable[int]) -> Iterable[int]:\n    banned_tokens = []\n\n    def _tokens_match(prev_tokens, tokens):\n        if len(tokens) == 0:\n            # if bad word tokens is just one token always ban it\n            return True\n        if len(tokens) > len(prev_input_ids):\n            # if bad word tokens are longer then prev input_ids they can't be equal\n            return False\n\n        if prev_tokens[-len(tokens) :] == tokens:\n            # if tokens match\n            return True\n        else:\n            return False\n\n    for prev_input_ids_slice in prev_input_ids:\n        banned_tokens_slice = []\n\n        for banned_token_seq in bad_words_ids:\n            assert len(banned_token_seq) > 0, \"Banned words token sequences {} cannot have an empty list\".format(\n                bad_words_ids\n            )\n\n            if _tokens_match(prev_input_ids_slice.tolist(), banned_token_seq[:-1]) is False:\n                # if tokens do not match continue\n                continue\n\n            banned_tokens_slice.append(banned_token_seq[-1])\n\n        banned_tokens.append(banned_tokens_slice)\n\n    return banned_tokens","metadata":{"execution":{"iopub.status.busy":"2023-05-31T21:05:30.324671Z","iopub.execute_input":"2023-05-31T21:05:30.325111Z","iopub.status.idle":"2023-05-31T21:05:30.433094Z","shell.execute_reply.started":"2023-05-31T21:05:30.325076Z","shell.execute_reply":"2023-05-31T21:05:30.431981Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# beam_search (baseline)\nlogger = logging.getLogger(__name__)\n\n\ndef main_no_neuro():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\"--model_name\", type=str, help=\"pretrained language model to use\", default=\"gpt2-medium\")\n    parser.add_argument(\"--input_path\", type=str, help=\"path of input file\", default=\"/kaggle/input/commonsenseqa-no-neuro-fs/nl_questions_fs.txt\")\n    parser.add_argument(\"--output_file\", type=str, help=\"output file\", default=\"/kaggle/working/baseline_few_shot_gpt2-medium.txt\")\n\n    parser.add_argument('--batch_size', type=int, default=8,\n                        help=\"Batch size for decoding.\")\n    parser.add_argument('--beam_size', type=int, default=8,\n                        help=\"Beam size for searching\")\n    parser.add_argument('--max_tgt_length', type=int, default=500,\n                        help=\"maximum length of decoded sentences\")\n    parser.add_argument('--min_tgt_length', type=int, default=1,\n                        help=\"minimum length of decoded sentences\")\n    parser.add_argument('--ngram_size', type=int, default=3,\n                        help='all ngrams can only occur once')\n    parser.add_argument('--length_penalty', type=float, default=1,\n                        help=\"length penalty for beam search\")\n    parser.add_argument('-f', type=str, default=10,\n                        help=\"optional early stop if all constraints are satisfied\")\n\n    args = parser.parse_args()\n\n    print(f\"Decoding with: {args.model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    model = AutoModelWithLMHead.from_pretrained(args.model_name)\n\n    free_gpu_cache()\n    model.eval()\n    model = model.to('cuda')\n\n    with open(args.input_path) as fin:\n        input_lines = [line.strip() for line in fin.read().splitlines()]\n    # TODO: TEMP\n#     input_lines = input_lines[:10]\n    input_lines = [tokenizer.tokenize(x) for x in input_lines]\n    input_lines = sorted(list(enumerate(input_lines)),\n                         key=lambda x: len(x[1]))\n    output_lines = [\"\"] * len(input_lines)\n    total_batch = math.ceil(len(input_lines) / args.batch_size)\n    next_i = 0\n\n    with tqdm(total=total_batch) as pbar:\n        while next_i < len(input_lines):\n            full_chunk = input_lines[next_i:next_i + args.batch_size]\n            prompt_tokens_num = sorted(set([len(x[1]) for x in full_chunk]))\n            step_len = args.batch_size\n            if len(prompt_tokens_num) > 1:\n                step_len = len([x for x in full_chunk if len(x[1]) == prompt_tokens_num[0]])\n\n            _chunk = input_lines[next_i:next_i + step_len]\n            buf_id = [x[0] for x in _chunk]\n            buf = [x[1] for x in _chunk]\n            next_i += step_len\n            input_ids = torch.stack([torch.from_numpy(np.array(tokenizer.convert_tokens_to_ids(x))) for x in buf])\n            max_gen_length = list(map(lambda x: x.shape[0], input_ids))\n            input_ids = input_ids.to('cuda')\n\n            outputs = model.generate(input_ids=input_ids,\n                                     min_length=args.min_tgt_length,\n                                     max_length=prompt_tokens_num[-1] + 10,\n                                     num_beams=args.beam_size,\n                                     no_repeat_ngram_size=args.ngram_size,\n                                     length_penalty=args.length_penalty)\n            prompt = [tokenizer.convert_tokens_to_string(x) for x in buf]\n            output_sequences = [prompt[i] + tokenizer.decode(o).split(prompt[i])[-1].split('<|endoftext|>')[0].rstrip()\n                                for i, o in enumerate(outputs)]\n\n            for i in range(len(buf)):\n                output_lines[buf_id[i]] = output_sequences[i].replace(\"\\n\", \"\")\n            pbar.update(1)\n\n    with open(args.output_file, \"w\", encoding=\"utf-8\") as fout:\n        for l in output_lines:\n            fout.write(l)\n            fout.write(\"\\n\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2023-05-31T21:05:30.434669Z","iopub.execute_input":"2023-05-31T21:05:30.435017Z","iopub.status.idle":"2023-05-31T21:05:30.454281Z","shell.execute_reply.started":"2023-05-31T21:05:30.434946Z","shell.execute_reply":"2023-05-31T21:05:30.453186Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"logger = logging.getLogger(__name__)\n\n\ndef main_neuro():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\"--model_name\", type=str, help=\"pretrained language model to use\", default='gpt2-medium')\n    parser.add_argument(\"--output_file\", type=str, help=\"output file\", default=\"/kaggle/working/neuro_few_shot_gpt2-medium.txt\")\n    parser.add_argument(\"--constraint_file\", type=str, help=\"constraint file\", default=\"/kaggle/input/constr/constraints_single_clause.json\")\n    parser.add_argument(\"--input_path\", type=str, help=\"initialization of decoding\", default='/kaggle/input/commonsenseqa-no-neuro-fs/nl_questions_fs.txt')\n\n    parser.add_argument('--batch_size', type=int, default=1,\n                        help=\"Batch size for decoding.\")\n    parser.add_argument('--beam_size', type=int, default=8,\n                        help=\"Beam size for searching\")\n    parser.add_argument('--max_tgt_length', type=int, default=32,\n                        help=\"maximum length of decoded sentences\")\n    parser.add_argument('--min_tgt_length', type=int, default=0,\n                        help=\"minimum length of decoded sentences\")\n    parser.add_argument('--ngram_size', type=int, default=3,\n                        help='all ngrams can only occur once')\n    parser.add_argument('--length_penalty', type=float, default=0.2,\n                        help=\"length penalty for beam search\")\n\n    parser.add_argument('--prune_factor', type=int, default=500000,\n                        help=\"fraction of candidates to keep based on score\")\n    parser.add_argument('--sat_tolerance', type=int, default=2,\n                        help=\"minimum satisfied clause of valid candidates\")\n    parser.add_argument('--beta', type=float, default=1.25,\n                        help=\"reward factor for in progress constraint\")\n    parser.add_argument('--early_stop', type=float, default=10,\n                        help=\"optional early stop if all constraints are satisfied\")\n    parser.add_argument('-f', type=str, default=10,\n                        help=\"optional early stop if all constraints are satisfied\")\n\n    args = parser.parse_args()\n    print(args)\n\n    print(f\"Decoding with: {args.model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    model = AutoModelWithLMHead.from_pretrained(args.model_name)\n\n    torch.cuda.empty_cache()\n    model.eval()\n    model = model.to('cuda')\n\n    period_id = [tokenizer.convert_tokens_to_ids('.')]\n    period_id.append(tokenizer.convert_tokens_to_ids('.'))\n    eos_ids = [tokenizer.eos_token_id] + period_id\n    PAD_ID = tokenizer.convert_tokens_to_ids('<pad>')\n    bad_token = [':', \"'\", '-', '_', '@', '', ':', 'who', \"'s\"]\n    bad_words_ids = [tokenizer.convert_tokens_to_ids([t]) for t in bad_token]\n\n    with open(args.input_path) as fin:\n        input_lines = [line.strip() for line in fin.read().splitlines()]\n\n    def read_constraints(file_name):\n        cons_list = []\n        with open(file_name, 'r') as f:\n            for i, line in enumerate(f):\n                cons = []\n                for concept in json.loads(line):\n                    cons.append([f' {c}' for c in concept if c.islower()])\n                cons_list.append(cons)\n        return cons_list\n\n    constraints_list = read_constraints(args.constraint_file)\n#     TODO: TEMP\n#     input_lines = input_lines[:50]\n#     for index, line in enumerate(input_lines):\n#         line = \"[SEP]\".join(line.split(\"[SEP]\")[2:])\n#         input_lines[index] = line\n    input_lines = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x)) for x in input_lines]\n    constraints_list = tokenize_constraints(tokenizer, constraints_list)\n\n    if path.exists(args.output_file):\n        count = len(open(args.output_file, 'r').readlines())\n        fout = Path(args.output_file).open(\"a\", encoding=\"utf-8\")\n        input_lines = input_lines[count:]\n        constraints_list = constraints_list[count:]\n    else:\n        fout = Path(args.output_file).open(\"w\", encoding=\"utf-8\")\n    total_batch = math.ceil(len(input_lines) / args.batch_size)\n\n    next_i = 0\n\n    with tqdm(total=total_batch) as pbar:\n        while next_i < len(input_lines):\n            _chunk = input_lines[next_i:next_i + args.batch_size]\n            constraints = init_batch(raw_constraints=constraints_list[next_i:next_i + args.batch_size],\n                                     beam_size=args.beam_size,\n                                     eos_id=eos_ids)\n            buf = _chunk\n            next_i += args.batch_size\n            \n            max_len = max([len(x) for x in buf])\n            buf = [x + [PAD_ID] * (max_len - len(x)) for x in buf]\n\n            input_ids = torch.stack([torch.from_numpy(np.array(x)) for x in buf])\n            input_ids = input_ids.to('cuda')\n            attention_mask = (~torch.eq(input_ids, PAD_ID)).int()\n            attention_mask = attention_mask.to('cuda')\n            \n            free_gpu_cache()\n            advanced_constraints = []\n            for j, init_cons in enumerate(constraints):\n                adv_cons = init_cons\n                for token in _chunk[j // args.beam_size]:\n                    adv_cons = adv_cons.advance(token)\n                advanced_constraints.append(adv_cons)\n            free_gpu_cache()\n            outputs, _, _ = generate(self=model,\n                                     input_ids=input_ids,\n                                     attention_mask=attention_mask,\n                                     pad_token_id=PAD_ID,\n                                     bad_words_ids=bad_words_ids,\n                                     min_length=args.min_tgt_length,\n                                     max_length=max_len + 10,\n                                     num_beams=args.beam_size,\n                                     no_repeat_ngram_size=args.ngram_size,\n                                     length_penalty=args.length_penalty,\n                                     constraints=advanced_constraints,\n                                     prune_factor=args.prune_factor,\n                                     sat_tolerance=args.sat_tolerance,\n                                     beta=args.beta,\n                                     early_stop=args.early_stop,\n                                     )\n\n            prompt = [tokenizer.decode(x) for x in buf]\n            output_sequences = [prompt[i] + tokenizer.decode(o).split(prompt[i])[-1].split('<|endoftext|>')[0].rstrip()\n                                for i, o in enumerate(outputs)]\n            for hypothesis in output_sequences:\n                fout.write(hypothesis.strip().replace('<|endoftext|>', '') + \"\\n\")\n                fout.flush()\n\n            pbar.update(1)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T21:18:05.689560Z","iopub.execute_input":"2023-05-31T21:18:05.690007Z","iopub.status.idle":"2023-05-31T21:18:05.727297Z","shell.execute_reply.started":"2023-05-31T21:18:05.689971Z","shell.execute_reply":"2023-05-31T21:18:05.726372Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# main_no_neuro()\n\nmain_neuro()","metadata":{"execution":{"iopub.status.busy":"2023-05-31T21:18:11.285767Z","iopub.execute_input":"2023-05-31T21:18:11.286132Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Namespace(model_name='gpt2-medium', output_file='/kaggle/working/neuro_few_shot_gpt2-medium.txt', constraint_file='/kaggle/input/constr/constraints_single_clause.json', input_path='/kaggle/input/commonsenseqa-no-neuro-fs/nl_questions_fs.txt', batch_size=1, beam_size=8, max_tgt_length=32, min_tgt_length=0, ngram_size=3, length_penalty=0.2, prune_factor=500000, sat_tolerance=2, beta=1.25, early_stop=10, f='/root/.local/share/jupyter/runtime/kernel-df34d823-5073-462a-84e2-4763e8c56939.json')\nDecoding with: gpt2-medium\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:1352: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  warnings.warn(\n 16%|        | 192/1221 [39:05<3:25:16, 11.97s/it]","output_type":"stream"}]},{"cell_type":"code","source":"# Clear output folder\nimport os\n\ndef remove_folder_contents(folder):\n    for the_file in os.listdir(folder):\n        file_path = os.path.join(folder, the_file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                remove_folder_contents(file_path)\n                os.rmdir(file_path)\n        except Exception as e:\n            print(e)\n\n# folder_path = '/kaggle/working'\n# remove_folder_contents(folder_path)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T21:16:52.792032Z","iopub.execute_input":"2023-05-31T21:16:52.792830Z","iopub.status.idle":"2023-05-31T21:16:52.799744Z","shell.execute_reply.started":"2023-05-31T21:16:52.792791Z","shell.execute_reply":"2023-05-31T21:16:52.798715Z"},"trusted":true},"execution_count":10,"outputs":[]}]}