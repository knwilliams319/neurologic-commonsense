{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 Fine-Tuning Loop\n",
    "**Last Edited On: 5/24/2023**<br>\n",
    "**Last Edited By: Kyle Williams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Necessary Imports\n",
    "'''\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "import pandas as pd # TODO: If we can eliminate pandas and save the file as a torch object, that would be nice\n",
    "import os # Won't be necessary when uploading this to Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load GPT2 and its corresponding tokenizer\n",
    "'''\n",
    "model_name = \"gpt2-medium\"  # You can also use \"gpt2\", \"gpt2-large\", or \"gpt2-xl\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, pad_token_id = tokenizer.eos_token_id)\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load Training Data and Encode it\n",
    "'''\n",
    "# Load the dev split (can't report data for test set without answers)\n",
    "cwd = os.getcwd()\n",
    "parent_path = '/'.join(cwd.split('/')[0:-1]) # removes the innermost folder (currently /experiments)\n",
    "train = pd.read_csv(parent_path + '/data/TRAIN.csv') # TODO: we need to create a file with our X/y data\n",
    "\n",
    "# TODO: if we eliminate pandas, we need to erase this call\n",
    "train = train.drop(columns = ['Unnamed: 0']) # the CSVs were saved with a leading index column that we can ignore\n",
    "\n",
    "max_length = None \n",
    "\n",
    "# Encoding the data\n",
    "encoded_dataset = tokenizer.batch_encode_plus(\n",
    "    dataset,  # Your downstream task dataset\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=max_length  # TODO: We need to find the maximum sequence length for our task because \n",
    "                           # this will make things much more efficient. This will depend on our decided upon prompt. \n",
    ")\n",
    "input_ids = torch.tensor(encoded_dataset[\"input_ids\"])\n",
    "attention_mask = torch.tensor(encoded_dataset[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Configure Training Hyperparameters\n",
    "'''\n",
    "# Stochastic Gradient Descent Hyperparameters\n",
    "num_epochs = None\n",
    "batch_size = None\n",
    "\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "# TODO: Replace with LinearLR somehow\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load Training Set for Torch Model (dependant on batch_size)\n",
    "'''\n",
    "train = torch.utils.data.TensorDataset(input_ids, attention_mask)\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training Loop\n",
    "'''\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # Move batch to device\n",
    "        inputs = batch.to(device)\n",
    "\n",
    "        # Clear gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = ...  # Compute your loss here\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update learning rate schedule\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print loss or other metrics if desired\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Step {step+1}/{len(train_dataloader)}, Loss: {loss.item()}\")\n",
    "\n",
    "# Save the trained model\n",
    "output_dir = \"fine_tuned_model/\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sslm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
