{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV to Prompt Data Pre-Processing\n",
    "**Last Edited On: 5/30/2023**<br>\n",
    "**Last Edited By: Kyle Williams**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation:** The code in this file takes a CSV of CommonsenseQA questions, and formats it into list of text containing the prompt and the question's corresponding answer. These experiment files should be easily uploaded to Colab so that the model's inference can be done on a GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Necessary Imports, Path Constants\n",
    "'''\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer\n",
    "import json\n",
    "\n",
    "READ_FOLDER = \"csv_splits/\"\n",
    "READ_FILES = [\"TRAINsplit\", \"DEVsplit\"] # ignore test set for now because it doesn't have answer labels\n",
    "INCLUDE_ANSWERS = [True, False] # we won't include the answer in the dev split's prompts\n",
    "WRITE_FOLDER = \"prompt_splits/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define a function to read the csv and format its contents into a tokenized prompt. \n",
    "Prompts will look like the following example:\n",
    "\n",
    "Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what? \n",
    "Choices: bank, library, department store, mall, new york.\n",
    "Answer: bank\n",
    "'''\n",
    "def question_to_prompt(csv_path, include_answer=True):\n",
    "    csv = pd.read_csv(READ_FOLDER + csv_path + \".csv\")\n",
    "    csv = csv.drop(columns = ['Unnamed: 0']) # the CSVs were saved with a leading index column that we can ignore\n",
    "\n",
    "    prompts = [\"\"] * csv.shape[0] #torch.zeros([csv.shape[0], tokenizer.max_len_single_sentence], dtype=torch.int32) # Vocab size is ~50000, which fits in uint16\n",
    "    #attention_masks = torch.zeros([csv.shape[0], tokenizer.max_len_single_sentence], dtype=torch.bool)\n",
    "    answers = [\"\"] * csv.shape[0]\n",
    "\n",
    "    for i, row in csv.iterrows():\n",
    "        prompt = \"Question: \" + row['question.stem'] + \"\\n\"\n",
    "        prompt += \"Choices: \"\n",
    "        \n",
    "        # Load the row. They were saved as strings, so this is a little wonky. I decided to use\n",
    "        # json.loads, which expects double quoted property keys. Since the question stem was saved\n",
    "        # as one huge json string with single quoted keys, we have to be careful to overwrite these \n",
    "        # without blindly overwriting single quotes in the choices (e.g. inside a contraction)\n",
    "        choices_str = row['question.choices']\n",
    "        choices_str = choices_str.replace(\"'label'\", '\"label\"')\n",
    "        choices_str = choices_str.replace(\"'text'\", '\"text\"')\n",
    "        choices_str = choices_str.replace('\"label\": \\'', '\"label\": \"')\n",
    "        choices_str = choices_str.replace('\"text\": \\'', '\"text\": \"')\n",
    "        choices_str = choices_str.replace('\\', \"text\"', '\", \"text\"')\n",
    "        choices_str = choices_str.replace('\\'}', '\"}')\n",
    "        choices = json.loads(choices_str)\n",
    "\n",
    "        answer_text = \"\"\n",
    "        for choice in choices: # Append the choices to the prompt\n",
    "            if choice['label'] == row['answerKey']:\n",
    "                answer_text = choice['text']\n",
    "\n",
    "            if choice['label'] == 'E':\n",
    "                #prompt += f\"or {choice['label']}: {choice['text']}. \" # includes label\n",
    "                prompt += f\"{choice['text']}.\\n\" # excludes label\n",
    "            else:\n",
    "                #prompt += f\"{choice['label']}: {choice['text']}, \" # includes label\n",
    "                prompt += f\"{choice['text']}, \"\n",
    "\n",
    "        if include_answer:\n",
    "            prompt += f\"Answer: {answer_text} <|endoftext|>\" # Add <|endoftext|> so fine-tuned model learns to end generation after it answers\n",
    "        else:\n",
    "            prompt += f\"Answer: \" # leave out the actual answer so the model may fill it in when freely generating\n",
    "        \n",
    "        prompts[i] = prompt\n",
    "        answers[i] = answer_text\n",
    "\n",
    "        # TODO: I should continue from here within the loop to tokenize the question keywords and connected concepts\n",
    "\n",
    "    with open(WRITE_FOLDER + csv_path + \"_prompts.pkl\", \"wb\") as file:\n",
    "        pickle.dump(prompts, file)\n",
    "    with open(WRITE_FOLDER + csv_path + \"_answers.pkl\", \"wb\") as file:\n",
    "        pickle.dump(answers, file)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Call the function on each of the splits to create new tensor files. \n",
    "'''\n",
    "for i, file in enumerate(READ_FILES):\n",
    "    # For now, use this cell to test output of the first file\n",
    "    question_to_prompt(file, include_answer=INCLUDE_ANSWERS[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sslm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
