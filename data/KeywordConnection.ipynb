{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Connection Pre-Processing\n",
    "**Last Edited On: 5/30/2023**<br>\n",
    "**Last Edited By: Kyle Williams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation:** The code in this file takes a CSV of CommonsenseQA questions, whose keywords have previously been extracted by BERT. For each question, ConceptNet is queried for its keywords to pull in potentially related concepts. These concept lists will be used to constrain the vocabulary of GPT2 during our experiments to hopefully improve its generation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Necessary Imports, Path Constants\n",
    "'''\n",
    "from ConceptNetRequestor import ConceptNetRequestor\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import pickle\n",
    "from functools import lru_cache\n",
    "\n",
    "# Constants\n",
    "CNR = ConceptNetRequestor()             # Our interface for querying data from ConceptNet\n",
    "READ_FOLDER = \"csv_splits/\"\n",
    "READ_FILES = [\"DEVsplit\"] #\"TRAINsplit\", \"DEVsplit\"] # ignore test set for now because it doesn't have answer labels\n",
    "WRITE_FOLDER = \"prompt_splits/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Script Hyperparameters\n",
    "'''\n",
    "DEPTH = 2 # Depth of ConceptNet edge traversal\n",
    "N_GRAMS = 2 # Query ConceptNet with combinations of keywords of this size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_keywords(keywords):\n",
    "    \"\"\"\n",
    "    Queries ConceptNet for the BERT-extracted keywords associated with a given question. \n",
    "    This creates a list of associated concepts from which to restrict the model's vocabulary\n",
    "    during decoding.\n",
    "    \"\"\"\n",
    "    vocab = []   # Stores keywords or traversed concepts we've already seen\n",
    "    depths = {}  # Stores at what depth we saw that keyword to speed up graph search\n",
    "\n",
    "    def add_to_vocab(keyword, curr_depth):\n",
    "        '''\n",
    "        Helper function to add 'keyword' to our vocabulary. Optionally explores edges of 'keyword' from ConceptNet\n",
    "        if 'depth' is nonzero. When this function adds 'keyword' to the vocabulary, it takes note of the index\n",
    "        of the row/column this keyword will be mapped to in the adjacency matrix to be created after. \n",
    "\n",
    "        NOTE: This function maintains the invariant that any item in 'vocab' is also in 'depths'\n",
    "        '''\n",
    "        # BASE CASES: Return early if these are hit\n",
    "        # CASE 1) If 'curr_depth' is 0, we are simply adding this node without traversing its edges\n",
    "        # CASE 2) If we've seen this keyword, check 'depths' to see if we've already done the work\n",
    "\n",
    "        # CASE ELSE) This keyword is not-yet-seen, we need to add it and find its edges because 'curr_depth' is nonzero\n",
    "        key = keyword.replace(\"_\", \" \")\n",
    "        if curr_depth == 0: # CASE 1\n",
    "            if keyword not in vocab:\n",
    "                vocab.append(key)\n",
    "                depths[keyword] = curr_depth\n",
    "            return\n",
    "        elif keyword in depths: # CASE 2\n",
    "            if depths[keyword] >= curr_depth: \n",
    "                return\n",
    "            else: \n",
    "                depths[keyword] = curr_depth # 'curr_depth' is greater than tracked depth, so we must update and do work\n",
    "        else: # CASE ELSE\n",
    "            vocab.append(key)\n",
    "            depths[keyword] = curr_depth\n",
    "\n",
    "        # RECURSIVE STEP: query ConceptNet for edges in/out of this node. Then add it and its connected concepts.\n",
    "        edges = CNR.get_edges(keyword) # Might be empty if this keyword is not an actual node in ConceptNet\n",
    "        if edges:\n",
    "            for edge in edges:         # Then add the connected concepts recursively\n",
    "                add_to_vocab(edge, curr_depth-1)\n",
    "        else: # since this is not an actual node in ConceptNet, we should not track it (may have been added by CASE ELSE)\n",
    "            vocab.remove(key)\n",
    "            del depths[keyword]\n",
    "\n",
    "\n",
    "    if N_GRAMS > 1: # If we want to try permutations of keywords\n",
    "        for n_gram in range(2, N_GRAMS+1): # Try all lengths of permutations specified\n",
    "            for combo in itertools.permutations(keywords, n_gram):\n",
    "                query_concept = '_'.join(combo) # Multi-word concepts are separated by '_' in the API path\n",
    "                add_to_vocab(query_concept, DEPTH)\n",
    "\n",
    "    for keyword in keywords: # Then process the original keywords without permutation\n",
    "        add_to_vocab(keyword, DEPTH)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in READ_FILES:\n",
    "    csv = pd.read_csv(READ_FOLDER + file + \".csv\")\n",
    "    csv = csv.drop(columns = ['Unnamed: 0']) # the CSVs were saved with a leading index column that we can ignore\n",
    "\n",
    "    q_concepts = [[]]*csv.shape[0] # will be a List[List[str]], populated with all concepts related to the keywords of each question\n",
    "\n",
    "    for i, row in csv.iterrows():\n",
    "        row_list = json.loads(row['keywords'].replace(\"'\", '\"'))\n",
    "        q_concepts[i] = connect_keywords(row_list)\n",
    "\n",
    "    with open(WRITE_FOLDER + file + \"_keywords.pkl\", \"wb\") as f:\n",
    "        pickle.dump(q_concepts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WRITE_FOLDER + file + \"_keywords.pkl\", \"rb\") as f:\n",
    "    my_list = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sslm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
