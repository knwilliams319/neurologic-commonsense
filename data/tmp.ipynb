{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2Tokenizer, GPT2LMHeadModel, T5Tokenizer, T5ForConditionalGeneration, pipeline, set_seed, PhrasalConstraint\n",
    "\n",
    "class BaseLM(torch.nn.Module):\n",
    "    def __init__(self, model:str = \"gpt2\", seed:int = 0, max_len:int = 15, num_returns:int = 1, num_beams:int = 5):\n",
    "        super(BaseLM, self).__init__()\n",
    "        if model == \"gpt2\":\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "            self.model = GPT2LMHeadModel.from_pretrained('gpt2-xl', pad_token_id=self.tokenizer.eos_token_id)\n",
    "            self.generator = pipeline('text-generation', model='gpt2-xl')\n",
    "        elif model == \"t5\":\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "            self.generator = None\n",
    "        else:\n",
    "            raise ValueError(f\"Model type ' {model} ' not supported. [BaseLM __init__()]\")\n",
    "        \n",
    "        set_seed(seed)\n",
    "        self.max_len = max_len\n",
    "        self.beams = num_beams\n",
    "        self.model_type = model\n",
    "        self.num_returns = num_returns\n",
    "    \n",
    "    def decode(self, text:str, constrained:bool = False, concepts:list = [], use_beam=True):\n",
    "        if self.generator:\n",
    "            if not constrained:\n",
    "                return self.generator(text, max_new_tokens=self.max_len, num_return_sequences=self.num_returns, do_sample=use_beam)\n",
    "            \n",
    "            print(\"Cannot perform constrained generation with generator. Generating manually.\")\n",
    "        \n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        \n",
    "        if constrained:\n",
    "            constraints = [\n",
    "                PhrasalConstraint(\n",
    "                    self.tokenizer(token, add_special_tokens=False).input_ids\n",
    "                )\n",
    "                for token in concepts\n",
    "            ]\n",
    "            \n",
    "            output = self.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                constraints=constraints,\n",
    "                num_beams=self.beams,\n",
    "                num_return_sequences=self.num_returns,\n",
    "                no_repeat_ngram_size=1,\n",
    "                remove_invalid_values=True,\n",
    "                do_sample=use_beam\n",
    "            )\n",
    "        else:\n",
    "            output = self.model.generate(inputs[\"input_ids\"], max_new_tokens=self.max_len, do_sample=use_beam)\n",
    "            \n",
    "        output_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return output_text\n",
    "\n",
    "# lm = BaseLM(model=\"gpt2\", max_len=100)\n",
    "# print(lm.decode(\"What is the third planet from the sun?\", constrained=True, concepts=[\"planet\", \"third\", \"sun\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = BaseLM('gpt2')\n",
    "dev = pd.read_csv('DEVsplit.csv')\n",
    "dev = dev.drop(columns = ['Unnamed: 0']) # the CSVs were saved with a leading index column that we can ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 718/718 [00:00<00:00, 538kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 9.56MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 9.44MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 11.5MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.52G/1.52G [01:02<00:00, 24.5MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 81.1kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\", pad_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Try standard prompting\n",
    "'''\n",
    "example_prompt = \"Question: The only baggage the woman checked was a drawstring bag, where was she heading with it?\\nChoices: garbage can, military, jewelry store, safe, airport\\nAnswer: airport\\n\"\n",
    "example_prompt += \"Question: Sammy wanted to go to where the people were. Where might he go?\\nChoices: race track, populated areas, the desert, apartment, roadblock\\nAnswer: populated areas\\n\"\n",
    "example_prompt += \"Question: To locate a choker not located in a jewelry box or boutique where would you go?\\nChoices: jewelry store, neck, jewelry box, boutique, bedroom\\nAnswer: jewelry store\\n\"\n",
    "example_prompt += \"Question: Google Maps and other highway and street GPS services have replaced what?\\nChoices: united states, mexico, countryside, atlas, oceans\\nAnswer: atlas\\n\"\n",
    "example_prompt += \"Question: The fox walked from the city into the forest, what was it looking for?\\nChoices: pretty flowers, hen house, natural habitat, storybook, dense forest\\nAnswer: natural habitat\\n\"\n",
    "example_prompt += \"Question: What home entertainment equipment requires cable?\\nChoices: radio shack, substation, cabinet, television, desk\\nAnswer: television\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Keep track of model's answers\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m answers \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(dev\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])]\n\u001b[1;32m      4\u001b[0m \u001b[39m# Query the model for each of its answers\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i, row \u001b[39min\u001b[39;00m dev\u001b[39m.\u001b[39miterrows():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dev' is not defined"
     ]
    }
   ],
   "source": [
    "# Keep track of model's answers\n",
    "answers = ['' for _ in range(dev.shape[0])]\n",
    "\n",
    "# Query the model for each of its answers\n",
    "for i, row in dev.iterrows():\n",
    "    '''\n",
    "    Create the prompt for the model. They will look like the following example (without a newline):\n",
    "\n",
    "    A revolving door is convenient for two direction travel, but it also serves as a security measure at a what? \n",
    "    A: bank. B: library. C: department store. D: mall. E: new york.\n",
    "    '''\n",
    "    prompt = example_prompt\n",
    "    prompt += \"Question: \" + row['question.stem'] + \"\\n\"\n",
    "    prompt += \"Choices: \"\n",
    "    \n",
    "    # Load the row. They were saved as strings, so this is a little wonky. I decided to use\n",
    "    # json.loads, which expects double quoted property keys. Since the question stem was saved\n",
    "    # as one huge json string with single quoted keys, we have to be careful to overwrite these \n",
    "    # without blindly overwriting single quotes in the choices (e.g. inside a contraction)\n",
    "    choices_str = row['question.choices']\n",
    "    choices_str = choices_str.replace(\"'label'\", '\"label\"')\n",
    "    choices_str = choices_str.replace(\"'text'\", '\"text\"')\n",
    "    choices_str = choices_str.replace('\"label\": \\'', '\"label\": \"')\n",
    "    choices_str = choices_str.replace('\"text\": \\'', '\"text\": \"')\n",
    "    choices_str = choices_str.replace('\\', \"text\"', '\", \"text\"')\n",
    "    choices_str = choices_str.replace('\\'}', '\"}')\n",
    "    choices = json.loads(choices_str)\n",
    "\n",
    "    for choice in choices: # Append the choices to the prompt\n",
    "        if choice['label'] == 'E':\n",
    "            #prompt += f\"or {choice['label']}: {choice['text']}. \" # includes label\n",
    "            prompt += f\"{choice['text']}\\n\" # excludes label\n",
    "        else:\n",
    "            #prompt += f\"{choice['label']}: {choice['text']}, \" # includes label\n",
    "            prompt += f\"{choice['text']}, \"\n",
    "    \n",
    "    prompt += \"Answer:\"\n",
    "\n",
    "    print('-'*100)\n",
    "    print(prompt)\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    # Is the open-end generation the problem?\n",
    "    answer = tokenizer.decode(model.generate(input_ids, \n",
    "                                             attention_mask=attention_mask,\n",
    "                                             pad_token_id=tokenizer.pad_token_id,\n",
    "                                             min_length=1,\n",
    "                                             max_length=input_ids.shape[1] + 50, \n",
    "                                             num_beams=10, \n",
    "                                             top_k=100,\n",
    "                                             top_p=0.90,\n",
    "                                             early_stopping=True,\n",
    "                                             do_sample=True)[0], \n",
    "                              skip_special_tokens=True)\n",
    "\n",
    "    print('-'*100)\n",
    "    print(answer)\n",
    "\n",
    "    break\n",
    "    print()\n",
    "    \n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurologic-commonsense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
